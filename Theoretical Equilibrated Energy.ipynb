{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOjVCG/4WBBBXCf9aZNGm1Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Theoretical Equilibrated Energy"],"metadata":{"id":"n2kNzdA3nIum"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GaRTRoyonDYY"},"outputs":[],"source":["#@title Installations\n","\n","\n","%%capture\n","!pip install equinox==0.11.2\n","!pip install diffrax==0.5.1\n","\n","!pip install torch==2.3.1\n","!pip install torchvision==0.18.1\n","\n","!pip install plotly==5.11.0\n","!pip install -U kaleido"]},{"cell_type":"code","source":["#@title Imports\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","from typing import Callable, Optional, Tuple, Dict\n","from jaxtyping import ArrayLike, Scalar, PyTree, Array, PRNGKeyArray\n","\n","import jax\n","from jax import vmap\n","import jax.random as jr\n","from jax.tree_util import tree_map\n","import jax.numpy as jnp\n","\n","import os\n","import random\n","import numpy as np\n","\n","import equinox as eqx\n","import equinox.nn as nn\n","from equinox import filter_grad\n","\n","from diffrax import (\n","    diffeqsolve,\n","    ODETerm,\n","    SaveAt,\n","    Heun,\n","    PIDController,\n","    AbstractSolver,\n","    AbstractStepSizeController\n",")\n","\n","import optax\n","from optax import (\n","    GradientTransformation,\n","    GradientTransformationExtraArgs,\n","    OptState\n",")\n","\n","import plotly.graph_objs as go"],"metadata":{"cellView":"form","id":"3gesBqxcnZCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title data utils\n","\n","\n","def get_dataloaders(dataset_id, batch_size):\n","    train_data = get_dataset(\n","        name=dataset_id,\n","        train=True,\n","        normalise=True\n","    )\n","    test_data = get_dataset(\n","        name=dataset_id,\n","        train=False,\n","        normalise=True\n","    )\n","    train_loader = DataLoader(\n","        dataset=train_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        drop_last=True\n","    )\n","    test_loader = DataLoader(\n","        dataset=test_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        drop_last=True\n","    )\n","    return train_loader, test_loader\n","\n","\n","def get_dataset(name, train, normalise):\n","    if name == \"MNIST\":\n","        dataset = MNIST(train, normalise)\n","    elif name == \"Fashion-MNIST\":\n","        dataset = FashionMNIST(train, normalise)\n","    elif name == \"CIFAR10\":\n","        dataset = CIFAR10(train, normalise)\n","    return dataset\n","\n","\n","class MNIST(datasets.MNIST):\n","    def __init__(self, train, normalise=True, save_dir=\"data\"):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.1307), std=(0.3081)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        img = torch.flatten(img)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","class FashionMNIST(datasets.FashionMNIST):\n","    def __init__(self, train, normalise=True, save_dir=\"data\"):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.5), std=(0.5)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        img = torch.flatten(img)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","class CIFAR10(datasets.CIFAR10):\n","    def __init__(self, train, normalise=True, save_dir=f\"data/CIFAR10\"):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.4914, 0.4822, 0.4465),\n","                        std=(0.247, 0.243, 0.261)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        img = torch.flatten(img)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","def one_hot(labels, n_classes=10):\n","    arr = torch.eye(n_classes)\n","    return arr[labels]\n"],"metadata":{"cellView":"form","id":"nuPXPnXUngDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title utils\n","\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","def make_linear_net(key, dataset, n_hidden, width):\n","    subkeys = jr.split(key, n_hidden+1)\n","    input_dim = 3072 if dataset == \"CIFAR10\" else 784\n","    output_dim = 10\n","    linear_net = []\n","    for l in range(n_hidden+1):\n","        linear_net.append(\n","            eqx.nn.Linear(\n","                in_features=input_dim if l == 0 else width,\n","                out_features=output_dim if l == n_hidden else width,\n","                key=subkeys[l],\n","                use_bias=False\n","            )\n","        )\n","    return linear_net\n","\n","\n","def compute_accuracy(truths: ArrayLike, preds: ArrayLike) -> Scalar:\n","    return jnp.mean(\n","        jnp.argmax(truths, axis=1) == jnp.argmax(preds, axis=1)\n","    )\n"],"metadata":{"cellView":"form","id":"OdAlpO8MrCUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title pc\n","\n","\n","def init_activities_with_ffwd(\n","        model: PyTree[Callable],\n","        x: ArrayLike\n",") -> PyTree[Array]:\n","\n","    activities = [vmap(model[0])(x)]\n","    for l in range(1, len(model)):\n","        activities.append(vmap(model[l])(activities[l-1]))\n","\n","    return activities\n","\n","\n","def pc_energy_fn(\n","        model: PyTree[Callable],\n","        activities: PyTree[ArrayLike],\n","        y: ArrayLike,\n","        x: Optional[ArrayLike] = None,\n","        record_layers: bool = False\n",") -> Scalar | Array:\n","    \"\"\"Computes the free energy for a feedforward neural network of the form\n","\n","    $$\n","    \\mathcal{F}(\\mathbf{z}; θ) = 1/N \\sum_i^N \\sum_{\\ell=1}^L || \\mathbf{z}_{i, \\ell} - f_\\ell(\\mathbf{z}_{i, \\ell-1}; θ) ||^2\n","    $$\n","\n","    given parameters $θ$, free activities $\\mathbf{z}$, output\n","    $\\mathbf{z}_L = \\mathbf{y}$ and optionally input $\\mathbf{z}_0 = \\mathbf{x}$.\n","    The activity of each layer $\\mathbf{z}_\\ell$ is some function of the previous\n","    layer, e.g. ReLU$(W_\\ell \\mathbf{z}_{\\ell-1} + \\mathbf{b}_\\ell)$\n","    for a fully connected layer with biases and ReLU as activation.\n","\n","    !!! note\n","\n","        The input x and output y correspond to the prior and observation of\n","        the generative model, respectively.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `activities`: List of activities for each layer free to vary.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    **Other arguments:**\n","\n","    - `record_layers`: If `True`, returns energies for each layer.\n","\n","    **Returns:**\n","\n","    The total or layer-wise energy normalised by batch size.\n","\n","    \"\"\"\n","    batch_size = y.shape[0]\n","    start_activity_l = 1 if x is not None else 2\n","    n_activity_layers = len(activities) - 1\n","    n_layers = len(model) - 1\n","\n","    eL = y - vmap(model[-1])(activities[-2])\n","    energies = [jnp.sum(eL ** 2)]\n","\n","    for act_l, net_l in zip(\n","            range(start_activity_l, n_activity_layers),\n","            range(1, n_layers)\n","    ):\n","        err = activities[act_l] - vmap(model[net_l])(activities[act_l-1])\n","        energies.append(jnp.sum(err ** 2))\n","\n","    e1 = activities[0] - vmap(model[0])(x) if (\n","            x is not None\n","    ) else activities[1] - vmap(model[0])(activities[0])\n","    energies.append(jnp.sum(e1 ** 2))\n","\n","    if record_layers:\n","        return jnp.array(energies) / batch_size\n","    else:\n","        return jnp.sum(jnp.array(energies)) / batch_size\n","\n","\n","def neg_activity_grad(\n","        t: float | int,\n","        activities: PyTree[ArrayLike],\n","        args: Tuple[PyTree[Callable], ArrayLike, Optional[ArrayLike]],\n","        energy_fn: Callable = pc_energy_fn,\n",") -> PyTree[Array]:\n","    \"\"\"Computes the negative gradient of the energy with respect to the activities $- \\partial \\mathcal{F} / \\partial \\mathbf{z}$.\n","\n","    This defines an ODE system to be integrated by `solve_pc_activities`.\n","\n","    **Main arguments:**\n","\n","    - `t`: Time step of the ODE system, used for downstream integration by\n","        `diffrax.diffeqsolve`.\n","    - `activities`: List of activities for each layer free to vary.\n","    - `args`: 3-Tuple with\n","        (i) list of callable layers for the generative model,\n","        (ii) network output (observation), and\n","        (iii) network input (prior).\n","    - `pc_energy_fn`: Free energy to take the gradient of.\n","\n","    **Returns:**\n","\n","    List of negative gradients of the energy w.r.t. the activities.\n","\n","    \"\"\"\n","    model, y, x = args\n","    dFdzs = jax.grad(energy_fn, argnums=1)(\n","        model,\n","        activities,\n","        y,\n","        x\n","    )\n","    return tree_map(lambda dFdz: -dFdz, dFdzs)\n","\n","\n","def compute_pc_param_grads(\n","        model: PyTree[Callable],\n","        activities: PyTree[ArrayLike],\n","        y: ArrayLike,\n","        x: Optional[ArrayLike] = None\n",") -> PyTree[Array]:\n","    \"\"\"Computes the gradient of the energy with respect to model parameters $\\partial \\mathcal{F} / \\partial θ$.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `activities`: List of activities for each layer free to vary.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    **Returns:**\n","\n","    List of parameter gradients for each network layer.\n","\n","    \"\"\"\n","    return filter_grad(pc_energy_fn)(\n","        model,\n","        activities,\n","        y,\n","        x\n","    )\n","\n","\n","def solve_pc_activities(\n","        model: PyTree[Callable],\n","        activities: PyTree[ArrayLike],\n","        y: ArrayLike,\n","        x: Optional[ArrayLike] = None,\n","        solver: AbstractSolver = Heun(),\n","        t1: int = 20,\n","        dt: float | int = None,\n","        stepsize_controller: AbstractStepSizeController = PIDController(\n","            rtol=1e-3, atol=1e-3\n","        ),\n","        record_iters: bool = False\n",") -> PyTree[Array]:\n","    \"\"\"Solves the activity (inference) dynamics of a predictive coding network.\n","\n","    This is a wrapper around `diffrax.diffeqsolve` to integrate the gradient\n","    ODE system `_neg_activity_grad` defining the PC activity dynamics\n","\n","    $$\n","    \\partial \\mathbf{z} / \\partial t = - \\partial \\mathcal{F} / \\partial \\mathbf{z}\n","    $$\n","\n","    where $\\mathcal{F}$ is the free energy, $\\mathbf{z}$ are the activities,\n","    with $\\mathbf{z}_L$ clamped to some target and $\\mathbf{z}_0$ optionally\n","    equal to some prior.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `activities`: List of activities for each layer free to vary.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    **Other arguments:**\n","\n","    - `solver`: Diffrax (ODE) solver to be used. Default is Heun, a 2nd order\n","        explicit Runge--Kutta method.\n","    - `t1`: Maximum end of integration region (20 by default).\n","    - `dt`: Integration step size. Defaults to None since the default\n","        `stepsize_controller` will automatically determine it.\n","    - `stepsize_controller`: diffrax controller for step size integration.\n","        Defaults to `PIDController`.\n","    - `record_iters`: If `True`, returns all integration steps.\n","\n","    **Returns:**\n","\n","    List with solution of the activity dynamics for each layer.\n","\n","    \"\"\"\n","    sol = diffeqsolve(\n","        terms=ODETerm(neg_activity_grad),\n","        solver=solver,\n","        t0=0,\n","        t1=t1,\n","        dt0=dt,\n","        y0=activities,\n","        args=(model, y, x),\n","        stepsize_controller=stepsize_controller,\n","        saveat=SaveAt(t1=True, steps=record_iters)\n","    )\n","    return sol.ys\n","\n","\n","def get_t_max(activities_iters: PyTree[Array]) -> Array:\n","    return jnp.argmax(activities_iters[0][:, 0, 0]) - 1\n","\n","\n","def compute_infer_energies(\n","        model: PyTree[Callable],\n","        activities_iters: PyTree[Array],\n","        t_max: Array,\n","        y: ArrayLike,\n","        x: Optional[ArrayLike] = None\n",") -> PyTree[Scalar]:\n","    \"\"\"Calculates layer energies during predictive coding inference.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `activities_iters`: Layer-wise activities at every inference iteration.\n","        Note that each set of activities will have 4096 steps as first\n","        dimension by diffrax default.\n","    - `t_max`: Maximum number of inference iterations to compute energies for.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    **Returns:**\n","\n","    List of layer-wise energies at every inference iteration.\n","\n","    \"\"\"\n","    def loop_body(state):\n","        t, energies_iters = state\n","\n","        energies = pc_energy_fn(\n","            model=model,\n","            activities=tree_map(lambda act: act[t], activities_iters),\n","            y=y,\n","            x=x,\n","            record_layers=True\n","        )\n","        energies_iters = energies_iters.at[:, t].set(energies)\n","        return t + 1, energies_iters\n","\n","    # 4096 is the max number of steps set in diffrax\n","    energies_iters = jnp.zeros((len(model), 4096))\n","    _, energies_iters = jax.lax.while_loop(\n","        lambda state: state[0] < t_max,\n","        loop_body,\n","        (0, energies_iters)\n","    )\n","    return energies_iters[::-1, :]\n","\n","\n","@eqx.filter_jit\n","def make_pc_step(\n","      model: PyTree[Callable],\n","      optim: GradientTransformation | GradientTransformationExtraArgs,\n","      opt_state: OptState,\n","      y: ArrayLike,\n","      x: Optional[ArrayLike] = None,\n","      ode_solver: AbstractSolver = Heun(),\n","      t1: int = 20,\n","      dt: float | int = None,\n","      stepsize_controller: AbstractStepSizeController = PIDController(\n","          rtol=1e-3, atol=1e-3\n","      ),\n","      key: Optional[PRNGKeyArray] = None,\n","      layer_sizes: Optional[PyTree[int]] = None,\n","      batch_size: Optional[int] = None,\n","      sigma: Scalar = 0.05,\n","      record_activities: bool = False,\n","      record_energies: bool = False\n",") -> Dict:\n","    \"\"\"Updates network parameters with predictive coding.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `optim`: Optax optimiser, e.g. `optax.sgd()`.\n","    - `opt_state`: State of Optax optimiser.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    !!! note\n","\n","        `key`, `layer_sizes` and `batch_size` must be passed if `input` is\n","        `None`, since unsupervised training will be assumed and activities need\n","        to be initialised randomly.\n","\n","    **Other arguments:**\n","\n","    - `ode_solver`: Diffrax ODE solver to be used. Default is Heun, a 2nd order\n","        explicit Runge--Kutta method.\n","    - `t1`: Maximum end of integration region (20 by default).\n","    - `dt`: Integration step size. Defaults to None since the default\n","        `stepsize_controller` will automatically determine it.\n","    - `stepsize_controller`: diffrax controller for step size integration.\n","        Defaults to `PIDController`.\n","    - `key`: `jax.random.PRNGKey` for random initialisation of activities.\n","    - `layer_sizes`: Dimension of all layers (input, hidden and output).\n","    - `batch_size`: Dimension of data batch for activity initialisation.\n","    - `sigma`: Standard deviation for Gaussian to sample activities from for\n","        random initialisation. Defaults to 5e-2.\n","    - `record_activities`: If `True`, returns activities at every inference\n","        iteration.\n","    - `record_energies`: If `True`, returns layer-wise energies at every\n","        inference iteration.\n","\n","    **Returns:**\n","\n","    Dict including model with updated parameters, optimiser, updated optimiser\n","    state, equilibrated activities, last inference step, MSE loss, and energies.\n","\n","    **Raises:**\n","\n","    - `ValueError` for inconsistent inputs.\n","\n","    \"\"\"\n","    if x is None and any(arg is None for arg in (key, layer_sizes, batch_size)):\n","        raise ValueError(\"\"\"\n","            If there is no input (i.e. `x` is None), then unsupervised training\n","            is assumed, and `key`, `layer_sizes` and `batch_size` must be\n","            passed for random initialisation of activities.\n","        \"\"\")\n","\n","    if record_energies:\n","        record_activities = True\n","\n","    activities = init_activities_with_ffwd(model=model, x=x)\n","\n","    mse_loss = jnp.mean((y - activities[-1])**2) if x is not None else None\n","    equilib_activities = solve_pc_activities(\n","        model=model,\n","        activities=activities,\n","        y=y,\n","        x=x,\n","        solver=ode_solver,\n","        t1=t1,\n","        dt=dt,\n","        stepsize_controller=stepsize_controller,\n","        record_iters=record_activities\n","    )\n","    t_max = get_t_max(equilib_activities) if record_activities else None\n","    energies = compute_infer_energies(\n","        model=model,\n","        activities_iters=equilib_activities,\n","        t_max=t_max,\n","        y=y,\n","        x=x\n","    ) if record_energies else None\n","\n","    param_grads = compute_pc_param_grads(\n","        model=model,\n","        activities=tree_map(\n","            lambda act: act[t_max if record_activities else jnp.array(0)],\n","            equilib_activities\n","        ),\n","        y=y,\n","        x=x\n","    )\n","    updates, opt_state = optim.update(\n","        updates=param_grads,\n","        state=opt_state,\n","        params=model\n","    )\n","    model = eqx.apply_updates(model=model, updates=updates)\n","    return {\n","        \"model\": model,\n","        \"optim\": optim,\n","        \"opt_state\": opt_state,\n","        \"activities\": equilib_activities,\n","        \"t_max\": t_max,\n","        \"loss\": mse_loss,\n","        \"energies\": energies\n","    }\n","\n","\n","@eqx.filter_jit\n","def test_discriminative_pc(\n","        model: PyTree[Callable],\n","        y: ArrayLike,\n","        x: ArrayLike,\n",") -> Scalar:\n","    \"\"\"Computes prediction accuracy of a discriminative predictive coding network.\n","\n","    **Main arguments:**\n","\n","    - `model`: List of callable model (e.g. neural network) layers.\n","    - `y`: Observation or target of the generative model.\n","    - `x`: Optional prior of the generative model.\n","\n","    **Returns:**\n","\n","    Accuracy of output predictions.\n","\n","    \"\"\"\n","    preds = init_activities_with_ffwd(model=model, x=x)[-1]\n","    return compute_accuracy(y, preds)\n"],"metadata":{"cellView":"form","id":"qAG8Np6KoCWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title analytical\n","\n","\n","def linear_equilib_energy_single(\n","        network: PyTree[nn.Linear],\n","        x: ArrayLike,\n","        y: ArrayLike\n",") -> Array:\n","    Ws = [l.weight for l in network]\n","    L = len(Ws)\n","\n","    # Compute product of weight matrices\n","    WLto1 = jnp.eye(Ws[-1].shape[0])\n","    for i in range(L - 1, -1, -1):\n","        WLto1 = WLto1 @ Ws[i]\n","\n","    # Compute rescaling\n","    S = jnp.eye(Ws[-1].shape[0])\n","    cumulative_prod = jnp.eye(Ws[-1].shape[0])\n","    for i in range(L - 1, 0, -1):\n","        cumulative_prod = cumulative_prod @ Ws[i]\n","        S += cumulative_prod @ cumulative_prod.T\n","\n","    # Compute full expression\n","    r = y - WLto1 @ x\n","    return r.T @ jnp.linalg.inv(S) @ r\n","\n","\n","@eqx.filter_jit\n","def linear_equilib_energy(\n","        network: PyTree[nn.Linear],\n","        x: ArrayLike,\n","        y: ArrayLike\n",") -> Array:\n","    \"\"\"Computes the theoretical equilibrated PC energy for a deep linear network (DLN).\n","\n","    $$\n","    \\mathcal{F}^* = 1/N \\sum_i^N (\\mathbf{y}_i - W_{L:1}\\mathbf{x}_i)^T S^{-1}(\\mathbf{y}_i - W_{L:1}\\mathbf{x}_i)\n","    $$\n","\n","    where the rescaling is $S = I_{d_y} + \\sum_{\\ell=2}^L (W_{L:\\ell})(W_{L:\\ell})^T$,\n","    and we use the shorthand $W_{L:\\ell} = W_L W_{L-1} \\dots W_\\ell$.\n","\n","    !!! note\n","\n","        This expression assumes no biases.\n","\n","    **Main arguments:**\n","\n","    - `network`: Linear network defined as a list of Equinox Linear layers.\n","    - `x`: Network input.\n","    - `y`: Network output.\n","\n","    **Returns:**\n","\n","    Mean total analytical energy across data batch.\n","\n","    \"\"\"\n","    return vmap(lambda x, y: linear_equilib_energy_single(\n","        network,\n","        x,\n","        y\n","    ))(x, y).mean()\n"],"metadata":{"cellView":"form","id":"A3UCvQoryMiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title train & test scripts\n","\n","\n","def evaluate(model, test_loader):\n","    test_acc = 0\n","    for batch_id, (img_batch, label_batch) in enumerate(test_loader):\n","        img_batch = img_batch.numpy()\n","        label_batch = label_batch.numpy()\n","\n","        test_acc += test_discriminative_pc(\n","            model=model,\n","            y=label_batch,\n","            x=img_batch\n","        )\n","\n","    return test_acc / len(test_loader)\n","\n","\n","def train(\n","      seed,\n","      dataset,\n","      n_hidden,\n","      width,\n","      lr,\n","      batch_size,\n","      t1,\n","      test_every,\n","      n_train_iters\n","):\n","    key = jr.PRNGKey(SEED)\n","    model = make_linear_net(key, dataset, n_hidden, width)\n","\n","    optim = optax.adam(lr)\n","    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n","    train_loader, test_loader = get_dataloaders(dataset, batch_size)\n","\n","    num_total_energies, theory_total_energies = [], []\n","    for iter, (img_batch, label_batch) in enumerate(train_loader):\n","        img_batch = img_batch.numpy()\n","        label_batch = label_batch.numpy()\n","\n","        theory_total_energies.append(\n","            linear_equilib_energy(\n","                network=model,\n","                x=img_batch,\n","                y=label_batch\n","            )\n","        )\n","        result = make_pc_step(\n","            model,\n","            optim,\n","            opt_state,\n","            y=label_batch,\n","            x=img_batch,\n","            t1=t1,\n","            record_energies=True\n","        )\n","        model, optim, opt_state = result[\"model\"], result[\"optim\"], result[\"opt_state\"]\n","        train_loss, t_max = result[\"loss\"], result[\"t_max\"]\n","        num_total_energies.append(result[\"energies\"][:, t_max-1].sum())\n","\n","        if ((iter+1) % test_every) == 0:\n","            avg_test_acc = evaluate(model, test_loader)\n","            print(\n","                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n","                f\"avg test accuracy={avg_test_acc:4f}\"\n","            )\n","            if (iter+1) >= n_train_iters:\n","                break\n","\n","    return {\n","        \"experiment\": jnp.array(num_total_energies),\n","        \"theory\": jnp.array(theory_total_energies)\n","    }\n"],"metadata":{"cellView":"form","id":"qKO5WoJBntLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title plotting\n","\n","\n","def plot_total_energies(energies, save_path):\n","    n_train_iters = len(energies[\"theory\"])\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    fig = go.Figure()\n","    for energy_type, energy in energies.items():\n","        is_theory = energy_type == \"theory\"\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=energy,\n","                name=energy_type,\n","                mode=\"lines\",\n","                line=dict(\n","                    width=3,\n","                    dash=\"dash\" if is_theory else \"solid\",\n","                    color=\"rgb(27, 158, 119)\" if is_theory else \"#00CC96\"\n","                ),\n","                legendrank=1 if is_theory else 2\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=450,\n","        xaxis=dict(\n","            title=\"Training iteration\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"Energy\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n"],"metadata":{"cellView":"form","id":"Qfd88NUEnkcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASETS = [\"MNIST\", \"Fashion-MNIST\", \"CIFAR10\"]\n","N_HIDDENS = [2, 5, 10]\n","\n","SEED = 0\n","RESULTS_DIR = \"results\"\n","WIDTH = 300\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 64\n","T1 = 300\n","TEST_EVERY = 10\n","N_TRAIN_ITERS = 100"],"metadata":{"id":"0PPawjDnneyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(SEED)\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","for dataset in DATASETS:\n","    for n_hidden in N_HIDDENS:\n","        print(f\"\\n{dataset}, {n_hidden} hidden layers\")\n","        energies = train(\n","            seed=SEED,\n","            dataset=dataset,\n","            n_hidden=n_hidden,\n","            width=WIDTH,\n","            lr=LEARNING_RATE,\n","            batch_size=BATCH_SIZE,\n","            test_every=TEST_EVERY,\n","            t1=T1,\n","            n_train_iters=N_TRAIN_ITERS\n","        )\n","        plot_total_energies(\n","            energies,\n","            f\"{RESULTS_DIR}/theory_energy_n_hidden_{dataset}_{n_hidden}.pdf\"\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iC7JgC7brKhP","executionInfo":{"status":"ok","timestamp":1721038036539,"user_tz":-60,"elapsed":320517,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}},"outputId":"ae843fd0-80c5-4f0d-8d32-100b3f3782d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","MNIST, 2 hidden layers\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 39194193.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 1220031.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 9705737.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4780559.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train iter 10, train loss=0.078076, avg test accuracy=0.752704\n","Train iter 20, train loss=0.062510, avg test accuracy=0.776342\n","Train iter 30, train loss=0.050910, avg test accuracy=0.780248\n","Train iter 40, train loss=0.052440, avg test accuracy=0.802684\n","Train iter 50, train loss=0.044833, avg test accuracy=0.829728\n","Train iter 60, train loss=0.045435, avg test accuracy=0.805188\n","Train iter 70, train loss=0.043837, avg test accuracy=0.798177\n","Train iter 80, train loss=0.056742, avg test accuracy=0.817508\n","Train iter 90, train loss=0.048744, avg test accuracy=0.810096\n","Train iter 100, train loss=0.045621, avg test accuracy=0.818409\n","\n","MNIST, 5 hidden layers\n","Train iter 10, train loss=0.068379, avg test accuracy=0.755609\n","Train iter 20, train loss=0.057660, avg test accuracy=0.773538\n","Train iter 30, train loss=0.058808, avg test accuracy=0.774740\n","Train iter 40, train loss=0.047768, avg test accuracy=0.793970\n","Train iter 50, train loss=0.051430, avg test accuracy=0.776743\n","Train iter 60, train loss=0.060327, avg test accuracy=0.784255\n","Train iter 70, train loss=0.051646, avg test accuracy=0.773938\n","Train iter 80, train loss=0.058102, avg test accuracy=0.783954\n","Train iter 90, train loss=0.049350, avg test accuracy=0.792869\n","Train iter 100, train loss=0.056579, avg test accuracy=0.795873\n","\n","MNIST, 10 hidden layers\n","Train iter 10, train loss=0.075101, avg test accuracy=0.586438\n","Train iter 20, train loss=0.075225, avg test accuracy=0.642328\n","Train iter 30, train loss=0.072230, avg test accuracy=0.707732\n","Train iter 40, train loss=0.062595, avg test accuracy=0.749299\n","Train iter 50, train loss=0.049505, avg test accuracy=0.795974\n","Train iter 60, train loss=0.049133, avg test accuracy=0.806891\n","Train iter 70, train loss=0.050118, avg test accuracy=0.799579\n","Train iter 80, train loss=0.045095, avg test accuracy=0.833834\n","Train iter 90, train loss=0.045795, avg test accuracy=0.815104\n","Train iter 100, train loss=0.046671, avg test accuracy=0.837440\n","\n","Fashion-MNIST, 2 hidden layers\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 13457452.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 272162.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5028931.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 7706023.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Train iter 10, train loss=0.059050, avg test accuracy=0.678185\n","Train iter 20, train loss=0.048410, avg test accuracy=0.718550\n","Train iter 30, train loss=0.047848, avg test accuracy=0.759115\n","Train iter 40, train loss=0.046312, avg test accuracy=0.763922\n","Train iter 50, train loss=0.048569, avg test accuracy=0.750801\n","Train iter 60, train loss=0.044502, avg test accuracy=0.771635\n","Train iter 70, train loss=0.053061, avg test accuracy=0.768530\n","Train iter 80, train loss=0.053993, avg test accuracy=0.747095\n","Train iter 90, train loss=0.037717, avg test accuracy=0.773638\n","Train iter 100, train loss=0.039773, avg test accuracy=0.772135\n","\n","Fashion-MNIST, 5 hidden layers\n","Train iter 10, train loss=0.059829, avg test accuracy=0.673978\n","Train iter 20, train loss=0.051413, avg test accuracy=0.724659\n","Train iter 30, train loss=0.056933, avg test accuracy=0.758413\n","Train iter 40, train loss=0.053957, avg test accuracy=0.766026\n","Train iter 50, train loss=0.055790, avg test accuracy=0.680889\n","Train iter 60, train loss=0.051597, avg test accuracy=0.760016\n","Train iter 70, train loss=0.043605, avg test accuracy=0.742488\n","Train iter 80, train loss=0.055559, avg test accuracy=0.763622\n","Train iter 90, train loss=0.044647, avg test accuracy=0.743189\n","Train iter 100, train loss=0.046224, avg test accuracy=0.744391\n","\n","Fashion-MNIST, 10 hidden layers\n","Train iter 10, train loss=0.078245, avg test accuracy=0.557592\n","Train iter 20, train loss=0.065944, avg test accuracy=0.558994\n","Train iter 30, train loss=0.060779, avg test accuracy=0.660557\n","Train iter 40, train loss=0.059200, avg test accuracy=0.713642\n","Train iter 50, train loss=0.057342, avg test accuracy=0.737580\n","Train iter 60, train loss=0.042115, avg test accuracy=0.748698\n","Train iter 70, train loss=0.045534, avg test accuracy=0.739684\n","Train iter 80, train loss=0.047730, avg test accuracy=0.752805\n","Train iter 90, train loss=0.042338, avg test accuracy=0.742488\n","Train iter 100, train loss=0.037400, avg test accuracy=0.769631\n","\n","CIFAR10, 2 hidden layers\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/CIFAR10/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 80706797.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/CIFAR10/cifar-10-python.tar.gz to data/CIFAR10\n","Files already downloaded and verified\n","Train iter 10, train loss=0.552155, avg test accuracy=0.115184\n","Train iter 20, train loss=0.258253, avg test accuracy=0.174279\n","Train iter 30, train loss=0.195730, avg test accuracy=0.203225\n","Train iter 40, train loss=0.137791, avg test accuracy=0.190605\n","Train iter 50, train loss=0.151842, avg test accuracy=0.193810\n","Train iter 60, train loss=0.145391, avg test accuracy=0.246394\n","Train iter 70, train loss=0.108184, avg test accuracy=0.223257\n","Train iter 80, train loss=0.111019, avg test accuracy=0.200621\n","Train iter 90, train loss=0.121446, avg test accuracy=0.232472\n","Train iter 100, train loss=0.105853, avg test accuracy=0.250501\n","\n","CIFAR10, 5 hidden layers\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Train iter 10, train loss=0.101535, avg test accuracy=0.186298\n","Train iter 20, train loss=0.105133, avg test accuracy=0.200321\n","Train iter 30, train loss=0.096691, avg test accuracy=0.216346\n","Train iter 40, train loss=0.093891, avg test accuracy=0.236178\n","Train iter 50, train loss=0.098259, avg test accuracy=0.234375\n","Train iter 60, train loss=0.104803, avg test accuracy=0.270132\n","Train iter 70, train loss=0.104926, avg test accuracy=0.263522\n","Train iter 80, train loss=0.099115, avg test accuracy=0.234876\n","Train iter 90, train loss=0.104408, avg test accuracy=0.238381\n","Train iter 100, train loss=0.106109, avg test accuracy=0.263421\n","\n","CIFAR10, 10 hidden layers\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Train iter 10, train loss=0.105370, avg test accuracy=0.184696\n","Train iter 20, train loss=0.108907, avg test accuracy=0.192909\n","Train iter 30, train loss=0.103339, avg test accuracy=0.177885\n","Train iter 40, train loss=0.099293, avg test accuracy=0.206931\n","Train iter 50, train loss=0.113013, avg test accuracy=0.239683\n","Train iter 60, train loss=0.101537, avg test accuracy=0.211939\n","Train iter 70, train loss=0.099759, avg test accuracy=0.256811\n","Train iter 80, train loss=0.097608, avg test accuracy=0.270333\n","Train iter 90, train loss=0.106110, avg test accuracy=0.266627\n","Train iter 100, train loss=0.099531, avg test accuracy=0.271034\n"]}]}]}
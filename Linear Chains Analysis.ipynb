{"cells":[{"cell_type":"markdown","metadata":{"id":"axaNFXcSONTc"},"source":["# Linear Chains Analysis\n","\n","This notebook compares the learning dynamics of backpropagation (BP) and predictive coding (PC) on linear multi-layer perceptrons with one and two hidden units (1- and 2-MLPs)."]},{"cell_type":"markdown","metadata":{"id":"QyrSXqGjORRE"},"source":["## Setup"]},{"cell_type":"code","execution_count":69,"metadata":{"cellView":"form","id":"XJrtVW4FkS9o","executionInfo":{"status":"ok","timestamp":1717439518298,"user_tz":-60,"elapsed":62966,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Installations\n","\n","\n","%%capture\n","#!sudo apt install nvidia-utils-515\n","!pip install plotly==5.11.0\n","!pip install -U kaleido\n","!pip install gif==3.0.0\n"]},{"cell_type":"code","execution_count":70,"metadata":{"cellView":"form","id":"v5ea-5AuUcjj","executionInfo":{"status":"ok","timestamp":1717439518299,"user_tz":-60,"elapsed":10,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Imports\n","\n","\n","import os\n","import random\n","import numpy as np\n","from typing import Tuple, Dict, Optional, List, Callable, Union\n","from numpy.polynomial.polynomial import Polynomial\n","\n","from jax import jacfwd, jacrev\n","from jax.numpy.linalg import eigh, norm\n","\n","import gif\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import LogNorm\n","\n","import plotly.express as px\n","from plotly.express.colors import sample_colorscale\n","import plotly.graph_objs as go\n","import plotly.figure_factory as ff"]},{"cell_type":"code","execution_count":71,"metadata":{"cellView":"form","id":"PCRSxTw_2XRp","executionInfo":{"status":"ok","timestamp":1717439518299,"user_tz":-60,"elapsed":8,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Utils\n","\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","def make_gaussian_dataset(mean, std, size):\n","    x = np.random.normal(loc=mean, scale=std, size=size)\n","    y = -x\n","    return (x, y)\n","\n","\n","def hessian(f):\n","    return jacfwd(jacrev(f))\n","\n","\n","def compute_theoretical_energy_eigenvals(weights, X, Y):\n","    n_weights = len(weights)\n","    theory_hessian_at_origin = np.zeros((n_weights, n_weights))\n","\n","    n_hidden = len(weights)-1\n","    theory_hessian_at_origin[-1, -1] = -(Y**2).mean()\n","    if n_hidden == 1:\n","        theory_hessian_at_origin[0, 1] = -(X*Y).mean()\n","        theory_hessian_at_origin[1, 0] = -(X*Y).mean()\n","\n","    theory_eigenvals, _ = eigh(theory_hessian_at_origin)\n","    return theory_eigenvals\n","\n","\n","def get_min_iter(lists):\n","    min_iter = 100000\n","    for i in lists:\n","        if len(i) < min_iter:\n","            min_iter = len(i)\n","    return min_iter\n","\n","\n","def get_min_iter_metric(metric):\n","    n_seeds = len(metric)\n","    min_iter = get_min_iter(lists=metric)\n","\n","    min_iter_metric = np.zeros((n_seeds, min_iter))\n","    for seed in range(n_seeds):\n","        min_iter_metric[seed, :] = metric[seed][:min_iter]\n","\n","    return min_iter_metric\n","\n","\n","def compute_metric_stats(metric):\n","    min_iter_metric = get_min_iter_metric(metric=metric)\n","    metric_means = min_iter_metric.mean(axis=0)\n","    metric_stds = min_iter_metric.std(axis=0)\n","    return metric_means, metric_stds\n"]},{"cell_type":"code","execution_count":72,"metadata":{"cellView":"form","id":"b2c_IC1CuV6L","executionInfo":{"status":"ok","timestamp":1717439518299,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Objective functions\n","\n","\n","def mse_loss_fun(ws, x, y):\n","    \"\"\"Calculates the mean squared error (MSE) loss for a neural chain, i.e. a\n","    network with one neuron in every layer.\"\"\"\n","    weight_prod = np.prod(ws)\n","    return ( 0.5 * (y - weight_prod*x)**2 ).mean()\n","\n","\n","def energy_fun(ws, xs, n_iters, dt):\n","    \"\"\"Runs iterative inference for a neural network with one or two hidden unit\n","    and returns the total energy.\"\"\"\n","    n_hidden = len(ws)-1\n","\n","    if n_hidden == 1:\n","        # initialisation\n","        Z = xs[1]\n","        e2 = xs[2] - ws[1]*Z\n","        e1 = Z - ws[0]*xs[0]\n","\n","        # iterative inference\n","        for t in range(n_iters):\n","            dZ = e1 - ws[1]*e2\n","            Z += - dZ * dt\n","\n","            e2 = xs[2] - ws[1]*Z\n","            e1 = Z - ws[0]*xs[0]\n","\n","    elif n_hidden == 2:\n","        # initialisation\n","        Z1 = xs[1]\n","        Z2 = xs[2]\n","\n","        e3 = xs[3] - ws[2]*Z2\n","        e2 = Z2 - ws[1]*Z1\n","        e1 = Z1 - ws[0]*xs[0]\n","\n","        # iterative inference\n","        for t in range(n_iters):\n","            dZ1 = e1 - ws[1]*e2\n","            dZ2 = e2 - ws[2]*e3\n","            Z1 += - dZ1 * DT\n","            Z2 += - dZ2 * DT\n","\n","            e3 = xs[3] - ws[2]*Z2\n","            e2 = Z2 - ws[1]*Z1\n","            e1 = Z1 - ws[0]*xs[0]\n","\n","    elif n_hidden == 5:\n","        # initialisation\n","        Z1 = xs[1]\n","        Z2 = xs[2]\n","        Z3 = xs[3]\n","        Z4 = xs[4]\n","        Z5 = xs[5]\n","\n","        e6 = xs[6] - ws[5]*Z5\n","        e5 = Z5 - ws[4]*Z4\n","        e4 = Z4 - ws[3]*Z3\n","        e3 = Z3 - ws[2]*Z2\n","        e2 = Z2 - ws[1]*Z1\n","        e1 = Z1 - ws[0]*xs[0]\n","\n","        # iterative inference\n","        for t in range(n_iters):\n","            dZ1 = e1 - ws[1]*e2\n","            dZ2 = e2 - ws[2]*e3\n","            dZ3 = e3 - ws[3]*e4\n","            dZ4 = e4 - ws[4]*e5\n","            dZ5 = e5 - ws[5]*e6\n","            Z1 += - dZ1 * DT\n","            Z2 += - dZ2 * DT\n","            Z3 += - dZ3 * DT\n","            Z4 += - dZ4 * DT\n","            Z5 += - dZ5 * DT\n","\n","            e6 = xs[6] - ws[5]*Z5\n","            e5 = Z5 - ws[4]*Z4\n","            e4 = Z4 - ws[3]*Z3\n","            e3 = Z3 - ws[2]*Z2\n","            e2 = Z2 - ws[1]*Z1\n","            e1 = Z1 - ws[0]*xs[0]\n","\n","    elif n_hidden == 10:\n","        # initialisation\n","        Z1 = xs[1]\n","        Z2 = xs[2]\n","        Z3 = xs[3]\n","        Z4 = xs[4]\n","        Z5 = xs[5]\n","        Z6 = xs[6]\n","        Z7 = xs[7]\n","        Z8 = xs[8]\n","        Z9 = xs[9]\n","        Z10 = xs[10]\n","\n","        e11 = xs[11] - ws[10]*Z10\n","        e10 = Z10 - ws[9]*Z9\n","        e9 = Z9 - ws[8]*Z8\n","        e8 = Z8 - ws[7]*Z7\n","        e7 = Z7 - ws[6]*Z6\n","        e6 = Z6 - ws[5]*Z5\n","        e5 = Z5 - ws[4]*Z4\n","        e4 = Z4 - ws[3]*Z3\n","        e3 = Z3 - ws[2]*Z2\n","        e2 = Z2 - ws[1]*Z1\n","        e1 = Z1 - ws[0]*xs[0]\n","\n","        # iterative inference\n","        for t in range(n_iters):\n","            dZ1 = e1 - ws[1]*e2\n","            dZ2 = e2 - ws[2]*e3\n","            dZ3 = e3 - ws[3]*e4\n","            dZ4 = e4 - ws[4]*e5\n","            dZ5 = e5 - ws[5]*e6\n","            dZ6 = e6 - ws[6]*e7\n","            dZ7 = e7 - ws[7]*e8\n","            dZ8 = e8 - ws[8]*e9\n","            dZ9 = e9 - ws[9]*e10\n","            dZ10 = e10 - ws[10]*e11\n","            Z1 += - dZ1 * DT\n","            Z2 += - dZ2 * DT\n","            Z3 += - dZ3 * DT\n","            Z4 += - dZ4 * DT\n","            Z5 += - dZ5 * DT\n","            Z6 += - dZ6 * DT\n","            Z7 += - dZ7 * DT\n","            Z8 += - dZ8 * DT\n","            Z9 += - dZ9 * DT\n","            Z10 += - dZ10 * DT\n","\n","            e11 = xs[11] - ws[10]*Z10\n","            e10 = Z10 - ws[9]*Z9\n","            e9 = Z9 - ws[8]*Z8\n","            e8 = Z8 - ws[7]*Z7\n","            e7 = Z7 - ws[6]*Z6\n","            e6 = Z6 - ws[5]*Z5\n","            e5 = Z5 - ws[4]*Z4\n","            e4 = Z4 - ws[3]*Z3\n","            e3 = Z3 - ws[2]*Z2\n","            e2 = Z2 - ws[1]*Z1\n","            e1 = Z1 - ws[0]*xs[0]\n","\n","    if n_hidden == 1:\n","        energy = (0.5*e1**2 + 0.5*e2**2).mean()\n","    elif n_hidden == 2:\n","        energy = (0.5*e1**2 + 0.5*e2**2 + 0.5*e3**2).mean()\n","    elif n_hidden == 5:\n","        energy = (0.5*e1**2 + 0.5*e2**2 + 0.5*e3**2 + 0.5*e4**2 + 0.5*e5**2 + 0.5*e6**2).mean()\n","    elif n_hidden == 10:\n","        energy = (0.5*e1**2 + 0.5*e2**2 + 0.5*e3**2 + 0.5*e4**2 + 0.5*e5**2 + 0.5*e6**2 + 0.5*e7**2 + 0.5*e8**2 + 0.5*e9**2 + 0.5*e10**2 + 0.5*e11**2).mean()\n","\n","    return energy\n"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"EzlNN8vk7EhC","cellView":"form","executionInfo":{"status":"ok","timestamp":1717439518299,"user_tz":-60,"elapsed":6,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Config\n","\n","\n","HIDDEN_UNITS = [1, 2, 5, 10]\n","N_SEEDS = 5\n","\n","RESULTS_DIR = \"results\"\n","\n","# dataset\n","DATA_MEAN, DATA_STD = 1., 0.1\n","BATCH_SIZE = 64\n","\n","# PC hyperparameters\n","N_ITERS = 20\n","DT = 0.1\n","\n","# optimization hyperparameters\n","WEIGHT_SCALE = 5e-2\n","LR = 0.4\n","\n","# landscape plotting\n","SAMPLING_RESOLUTION = 30\n","COLORSCALE = \"RdBu_r\"\n","GRADIENT_FIELD_SCALE = 0.1\n","PLOT_INFER_ITERATIONS = [0, 1, 5, 10, 20, 30, 40, 50, 100]\n"]},{"cell_type":"code","execution_count":74,"metadata":{"cellView":"form","id":"mDZ4iYcV66Ak","executionInfo":{"status":"ok","timestamp":1717439518979,"user_tz":-60,"elapsed":686,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Landscape plotting\n","\n","\n","@gif.frame\n","def plot_objective_contour(\n","        objective_mesh: np.ndarray,\n","        weights: Tuple[np.ndarray, np.ndarray],\n","        vector_field: np.ndarray,\n","        vector_field_scale: float,\n","        objective_name: str,\n","        title: str,\n","        save_path: str,\n","        weight_updates: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n","        smooth_contours: bool = True\n","    ):\n","    \"\"\"Plots the contours or level sets of a given 2D objective or cost function\n","    and its gradient as a superimposed vector field.\n","\n","    The gradient field is standardised and rescaled for easier visualisation.\"\"\"\n","\n","    contours_coloring = \"heatmap\" if smooth_contours else \"fill\"\n","\n","    # contour plot\n","    contour = go.Contour(\n","        z=objective_mesh,\n","        x=weights[0],\n","        y=weights[1],\n","        colorscale=COLORSCALE,\n","        showscale=False,\n","        contours_coloring=contours_coloring\n","    )\n","\n","    # gradient vector field\n","    vector_field = (vector_field - vector_field.mean()) / vector_field.std()\n","    w1_mesh, w2_mesh = np.meshgrid(weights[0], weights[1])\n","    quiver = ff.create_quiver(\n","        x=w1_mesh,\n","        y=w2_mesh,\n","        u=vector_field[:, :, 0],\n","        v=vector_field[:, :, 1],\n","        marker_color=\"rgb(255, 255, 51)\",\n","        opacity=0.9,\n","        scale=vector_field_scale,\n","        line_width=0.8,\n","        showlegend=False\n","    )\n","    fig = go.FigureWidget(data=[contour, quiver.data[0]])\n","\n","    # colorbar\n","    objective = \"\\mathcal{L}\" if objective_name == \"loss\" else \"\\mathcal{F}\"\n","    max_objective, min_objective = objective_mesh.max(), objective_mesh.min()\n","    colorbar_trace = go.Scatter(\n","        x=[None],\n","        y=[None],\n","        mode=\"markers\",\n","        showlegend=False,\n","        marker=dict(\n","            colorscale=COLORSCALE,\n","            showscale=True,\n","            cmin=min_objective,\n","            cmax=max_objective,\n","            colorbar=dict(\n","                title=f\"$\\LARGE{{{objective}}}$\",\n","                len=0.5,\n","                title_side=\"right\",\n","                tickfont=dict(size=16),\n","                tickvals=[min_objective, max_objective],\n","                ticktext=[\"Low\", \"High\"]\n","            )\n","        ),\n","        hoverinfo=\"none\"\n","    )\n","    fig.add_trace(colorbar_trace)\n","\n","    # example training trajectory\n","    if weight_updates is not None:\n","        marker_color = \"rgb(255, 255, 51)\"\n","\n","        n_updates = len(weight_updates[0])\n","        for t in range(n_updates):\n","            if t == 0:\n","                text = [\"$\\huge{{w^0}}$\"]\n","                symbol = \"diamond\"\n","                size = 10\n","            elif t == (n_updates-1):\n","                text = [\"$\\huge{{w^*}}$\"]\n","                symbol = \"cross\"\n","                size = 13\n","            else:\n","                text = []\n","                symbol = \"circle\"\n","                size = 4\n","\n","            fig.add_traces(\n","                go.Scatter(\n","                    x=[weight_updates[0][t]],\n","                    y=[weight_updates[1][t]],\n","                    mode=\"markers+text\",\n","                    marker=dict(size=size, color=marker_color, symbol=symbol),\n","                    showlegend=False,\n","                    text=text,\n","                    textposition=\"top right\",\n","                    textfont=dict(color=marker_color)\n","                )\n","            )\n","            fig.update_layout(\n","                title=dict(\n","                    text=title,\n","                    y=0.85,\n","                    x=0.1,\n","                    xanchor=\"left\",\n","                    yanchor=\"top\"\n","                ),\n","                xaxis=dict(title=\"$\\LARGE{w_1}$\", showticklabels=False),\n","                yaxis=dict(title=\"$\\LARGE{w_2}$\", showticklabels=False),\n","                font=dict(size=16),\n","                plot_bgcolor=\"white\",\n","                width=500,\n","                height=400,\n","                margin=dict(r=100, b=50, l=50, t=80)\n","            )\n","            fig.write_image(f\"{save_path}_train_iter_{t}.pdf\")\n","\n","    else:\n","        fig.update_layout(\n","            title=dict(\n","                text=title,\n","                y=0.85,\n","                x=0.1,\n","                xanchor=\"left\",\n","                yanchor=\"top\"\n","            ),\n","            xaxis=dict(title=\"$\\LARGE{w_1}$\", showticklabels=False),\n","            yaxis=dict(title=\"$\\LARGE{w_2}$\", showticklabels=False),\n","            font=dict(size=16),\n","            plot_bgcolor=\"white\",\n","            width=500,\n","            height=400,\n","            margin=dict(r=100, b=50, l=50, t=80)\n","        )\n","        fig.write_image(save_path)\n","        return fig\n","\n","\n","@gif.frame\n","def plot_objective_surface(\n","        objective_mesh: np.ndarray,\n","        weights: Tuple[np.ndarray, np.ndarray],\n","        save_path: str,\n","        weight_updates: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n","        train_losses: Optional[List[float]] = None\n","    ) -> go.Figure():\n","    \"\"\"Plots and saves a given 2D objective or cost function as a surface.\"\"\"\n","\n","    fig = go.Figure(\n","        data=go.Surface(\n","            z=objective_mesh,\n","            x=weights[0],\n","            y=weights[1],\n","            colorscale=COLORSCALE,\n","        )\n","    )\n","    fig.update_traces(\n","        contours_z=dict(\n","            show=True,\n","            usecolormap=True,\n","            highlightcolor=\"limegreen\",\n","            project_z=True\n","        ),\n","        showscale=False\n","    )\n","\n","    # example weight trajectory\n","    if weight_updates is not None:\n","        fig.add_scatter3d(\n","            x=weight_updates[0],\n","            y=weight_updates[1],\n","            z=np.array(train_losses)+4e-2,\n","            mode=\"markers\",\n","            marker=dict(\n","                line=dict(width=2, color=\"black\"),\n","                size=5,\n","                color=\"rgb(255, 255, 51)\"\n","            )\n","        )\n","\n","    fig.update_layout(\n","        scene=dict(\n","            xaxis=dict(\n","                title=\"\",\n","                nticks=3,\n","                autorange=\"reversed\"\n","            ),\n","            yaxis=dict(\n","                title=\"\",\n","                nticks=3,\n","                autorange=\"reversed\"\n","            ),\n","            zaxis=dict(\n","                title=\"\",\n","                showticklabels=False\n","            )\n","        ),\n","        scene_camera=dict(\n","            center=dict(x=0.05, y=0.1, z=0),\n","            eye=dict(x=0.75, y=1.8, z=1.25)\n","        ),\n","        font=dict(size=16),\n","        height=600,\n","        width=700,\n","        scene_aspectmode=\"cube\"\n","    )\n","    fig.write_image(save_path)\n","    return fig\n","\n","\n","@gif.frame\n","def plot_objective_volume(\n","      objective_mesh: np.ndarray,\n","      input_domain: Union[int, float],\n","      sampling_resolution: int,\n","      save_path: str,\n","      weight_updates: Optional[List[np.ndarray]] = None\n","    ) -> go.Figure():\n","    \"\"\"Plots and saves a given 3D objective or cost function as a volume plot.\"\"\"\n","\n","    w1s, w2s, w3s = np.mgrid[\n","        -input_domain:input_domain:complex(sampling_resolution),\n","        -input_domain:input_domain:complex(sampling_resolution),\n","        -input_domain:input_domain:complex(sampling_resolution)\n","    ]\n","\n","    fig = go.Figure()\n","    fig.add_traces(go.Volume(\n","        x=w1s.flatten(),\n","        y=w2s.flatten(),\n","        z=w3s.flatten(),\n","        value=objective_mesh.flatten(),\n","        opacity=0.5,\n","        surface_count=20,\n","        colorscale=COLORSCALE\n","    ))\n","    fig.update_traces(showscale=False)\n","\n","    if weight_updates is not None:\n","        marker_color = \"rgb(255, 255, 51)\"\n","        marker_opacity = 0.2\n","        n_updates = len(weight_updates[0])\n","        for t in range(n_updates):\n","\n","            if t == 0:\n","                symbol = \"diamond\"\n","                size = 8\n","            elif t == (n_updates-1):\n","                symbol = \"cross\"\n","                size = 12\n","            else:\n","                text = []\n","                symbol = \"circle\"\n","                size = 6\n","\n","            #if t % 5 == 0:\n","            fig.add_traces(go.Scatter3d(\n","                x=[weight_updates[0][t]],\n","                y=[weight_updates[1][t]],\n","                z=[weight_updates[2][t]],\n","                mode=\"markers\",\n","                marker=dict(\n","                  size=size,\n","                  color=marker_color,\n","                  opacity=marker_opacity,\n","                  symbol=symbol\n","                ),\n","                showlegend=False\n","            ))\n","\n","            fig.update_layout(\n","                scene=dict(\n","                    xaxis=dict(\n","                        title=\"\",\n","                        nticks=3,\n","                        autorange=\"reversed\"\n","                    ),\n","                    yaxis=dict(\n","                        title=\"\",\n","                        nticks=3,\n","                        autorange=\"reversed\"\n","                    ),\n","                    zaxis=dict(\n","                        title=\"\",\n","                        showticklabels=False\n","                    )\n","                ),\n","                scene_camera=dict(eye=dict(x=2.5, y=1.25, z=1.25)),\n","                font=dict(size=16),\n","                height=800,\n","                width=800,\n","                scene_aspectmode=\"data\"\n","            )\n","            fig.write_image(f\"{save_path}_train_iter_{t}.pdf\")\n","\n","    else:\n","        fig.update_layout(\n","            scene=dict(\n","                xaxis=dict(\n","                    title=\"\",\n","                    nticks=3,\n","                    autorange=\"reversed\",\n","                    showticklabels=False,\n","                    showbackground=False\n","                ),\n","                yaxis=dict(\n","                    title=\"\",\n","                    nticks=3,\n","                    autorange=\"reversed\",\n","                    showticklabels=False,\n","                    showbackground=False\n","                ),\n","                zaxis=dict(\n","                    title=\"\",\n","                    showticklabels=False,\n","                    showbackground=False\n","                )\n","            ),\n","            scene_camera=dict(eye=dict(x=2.5, y=1.25, z=1.25)),\n","            font=dict(size=16),\n","            height=800,\n","            width=800,\n","            scene_aspectmode=\"cube\"\n","        )\n","        fig.write_image(save_path)\n","        return fig\n","\n"]},{"cell_type":"code","execution_count":75,"metadata":{"cellView":"form","id":"BYJP_pSywgIG","executionInfo":{"status":"ok","timestamp":1717439518979,"user_tz":-60,"elapsed":5,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Plotting\n","\n","\n","def plot_losses(losses: Dict, save_path: str) -> None:\n","    \"\"\"Plots and saves train and test losses.\"\"\"\n","    n_train_iters = len(losses[\"train\"])\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    fig = go.Figure()\n","    for loss_type, loss in losses.items():\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=loss,\n","                name=loss_type,\n","                mode=\"lines\",\n","                line=dict(width=3)\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=300,\n","        xaxis=dict(\n","            title=\"Training iteration\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\LARGE{\\mathcal{L}}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_energies(\n","        energies: List[np.ndarray],\n","        n_infer_iters: int,\n","        save_path: str,\n","        show_tot: bool = False\n","    ) -> None:\n","    \"\"\"Plots and saves train and test energies.\"\"\"\n","    n_hidden = len(energies)-1\n","\n","    train_time = len(energies[0])\n","    time_points = [b+1 for b in range(train_time)]\n","    n_train_iters = int(train_time / n_infer_iters+1)\n","    ts = [((n_infer_iters+1)*train_iter - 1) for train_iter in range(1, n_train_iters-1)]\n","\n","    fig = go.Figure()\n","    colors = [\"#636EFA\", \"#EF553B\", \"#00CC96\"]\n","    for i, (energy, color) in enumerate(zip(energies, colors)):\n","        fig.add_traces(\n","            go.Scatter(\n","                x=time_points,\n","                y=energy,\n","                name=f\"$\\mathcal{{F}}_{{{i+1}}}$\",\n","                mode=\"lines\",\n","                line=dict(width=2, color=color)\n","            )\n","        )\n","\n","    if show_tot:\n","        color = \"#FFA15A\"\n","        fig.add_traces(\n","            go.Scatter(\n","                x=time_points,\n","                y=energies[0]+energies[1],\n","                name=\"$\\mathcal{F}_{tot}$\",\n","                line=dict(width=2, color=color)\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=400,\n","        margin=dict(\n","            l=50,\n","            r=50,\n","            b=100,\n","            t=100,\n","            pad=4\n","        ),\n","        xaxis=dict(\n","            title=\"Iteration\",\n","            tickvals=[1, int(time_points[-1]/2), time_points[-1]],\n","            ticktext=[1, int(time_points[-1]/2), time_points[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\LARGE{\\mathcal{F}}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_updates(updates: List[np.ndarray], update_type: str, save_path: str) -> None:\n","    \"\"\"Plots and saves updates.\"\"\"\n","    n_weights = len(updates)\n","    n_train_iters = len(updates[0])\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    fig = go.Figure()\n","    names = [f\"$w_{i+1}$\" if update_type == \"weights\" else f\"$\\partial w_{i+1}$\" for i in range(n_weights)]\n","    colors = [\"#AB36FA\", \"#FFA15A\", \"#19D3F3\"]\n","    for update, name, color in zip(updates, names, colors):\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=update,\n","                name=name,\n","                mode=\"lines\",\n","                line=dict(width=3, color=color)\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=300,\n","        xaxis=dict(\n","            title=\"Training iteration\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"Value\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n","\n","\n","@gif.frame\n","def plot_predictions(\n","        targets: np.ndarray,\n","        predictions: np.ndarray,\n","        title: Optional[str] = None,\n","        save_path: Optional[str] = None\n","    ) -> go.Figure():\n","    \"\"\"Plots and saves predictions against targets.\"\"\"\n","\n","    n_example = len(targets)\n","    examples = [i+1 for i in range(n_example)]\n","\n","    fig = go.Figure()\n","    fig.add_traces(\n","        go.Scatter(\n","            x=examples,\n","            y=targets,\n","            name=\"true\",\n","            mode=\"lines\",\n","            line=dict(width=3)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            x=examples,\n","            y=predictions,\n","            mode=\"lines\",\n","            name=\"prediction\",\n","            line=dict(width=3)\n","        )\n","    )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=600,\n","        xaxis=dict(\n","            title=\"Data\",\n","            tickvals=[1, int(examples[-1]/2), examples[-1]],\n","            ticktext=[1, int(examples[-1]/2), examples[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\LARGE{y}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    if title is not None:\n","        fig.update_layout(\n","            title=dict(\n","                text=title,\n","                y=0.82,\n","                x=0.45,\n","                xanchor=\"center\",\n","                yanchor=\"top\"\n","            )\n","        )\n","    if save_path is not None:\n","        fig.write_image(save_path)\n","\n","    return fig\n","\n","\n","@gif.frame\n","def plot_hessian_matrix(hessian_matrix, save_path, title=None):\n","    fig, ax = plt.subplots()\n","    heatmap = ax.imshow(\n","        X=hessian_matrix,\n","        cmap=\"bwr\",\n","        vmin=-1,\n","        vmax=1\n","    )\n","    cbar = fig.colorbar(heatmap, ax=ax, location=\"right\", ticks=[-1, 0, 1])\n","    cbar.ax.tick_params(labelsize=25)\n","    ticks = np.arange(len(hessian_matrix), dtype=int)\n","    ax.set_xticks(ticks)\n","    ax.set_yticks(ticks)\n","    ax.set_xticklabels(ticks+1)\n","    ax.set_yticklabels(ticks+1)\n","\n","    if title is not None:\n","        plt.title(title, fontsize=20)\n","    fig.savefig(save_path)\n","    return fig\n","\n","\n","def plot_loss_and_energy_hessian_eigenvals(hessian_eigenvals: List, save_path: str) -> None:\n","    fig = go.Figure()\n","    names = [\"loss\", \"energy (numeric)\", \"energy (theory)\"]\n","    colors = [\"#EF553B\", \"#636EFA\", \"rgba(0,0,0,0)\"]\n","    for eigenval, name, color in zip(hessian_eigenvals, names, colors):\n","        fig.add_trace(\n","            go.Histogram(\n","                x=eigenval,\n","                histnorm=\"probability\",\n","                nbinsx=10,\n","                name=name,\n","                marker=dict(\n","                    color=color,\n","                    line=dict(\n","                        color=\"black\",\n","                        width=2 if \"theory\" in name else 0\n","                    )\n","                ),\n","            )\n","        )\n","\n","    fig.update_layout(\n","        barmode=\"overlay\",\n","        height=360,\n","        width=525,\n","        title=dict(\n","            y=0.75,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\"\n","        ),\n","        xaxis=dict(title=\"Hessian eigenvalue\"),\n","        yaxis=dict(\n","            title=f\"Density (log)\",\n","            type=\"log\",\n","            exponentformat=\"power\",\n","            dtick=1\n","        ),\n","        legend=dict(\n","            yanchor=\"top\",\n","            y=0.99,\n","            xanchor=\"left\",\n","            x=0.01\n","        ),\n","        font=dict(size=18)\n","    )\n","    fig.update_layout(\n","        legend=dict(\n","            yanchor=\"top\",\n","            y=0.99,\n","            xanchor=\"left\",\n","            x=0.01,\n","            font=dict(size=16)\n","        )\n","    )\n","    fig.update_traces(opacity=0.75)\n","    fig.write_image(save_path)\n","\n","\n","def plot_bp_and_pc_loss_stats(\n","        means: Tuple[np.ndarray],\n","        stds: Tuple[np.ndarray],\n","        loss_title: str,\n","        save_path: str\n","    ) -> None:\n","    max_train_iter = len(means[0]) if len(means[0]) >= len(means[1]) else len(means[1])\n","\n","    fig = go.Figure()\n","    for i in range(2):\n","        n_train_iters = len(means[i])\n","        train_iters = [b+1 for b in range(n_train_iters)]\n","\n","        color = \"#EF553B\" if i == 0 else \"#636EFA\"\n","        y_upper, y_lower = means[i] + stds[i], means[i] - stds[i]\n","\n","        fig.add_traces(\n","            go.Scatter(\n","                x=list(train_iters)+list(train_iters[::-1]),\n","                y=list(y_upper)+list(y_lower[::-1]),\n","                fill=\"toself\",\n","                fillcolor=color,\n","                line=dict(color=\"rgba(255,255,255,0)\"),\n","                hoverinfo=\"skip\",\n","                showlegend=False,\n","                opacity=0.3\n","            )\n","        )\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=means[i],\n","                name=\"BP\" if i == 0 else \"PC\",\n","                mode=\"lines+markers\",\n","                line=dict(width=3, color=color)\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        xaxis=dict(\n","            title=\"Training iteration\",\n","            tickvals=[1, int(max_train_iter/2), max_train_iter],\n","            ticktext=[1, int(max_train_iter/2), max_train_iter],\n","        ),\n","        yaxis=dict(\n","            title=loss_title,\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_bp_vs_pc_grad_norm_stats(\n","        means: Tuple[np.ndarray],\n","        stds: Tuple[np.ndarray],\n","        save_path: str\n","    ) -> None:\n","    max_train_iter = len(means[0]) if len(means[0]) >= len(means[1]) else len(means[1])\n","\n","    fig = go.Figure()\n","    for i in range(2):\n","        n_train_iters = len(means[i])\n","        train_iters = [b+1 for b in range(n_train_iters)]\n","\n","        color = \"#EF553B\" if i == 0 else \"#636EFA\"\n","        y_upper, y_lower = means[i] + stds[i], means[i] - stds[i]\n","\n","        fig.add_traces(\n","            go.Scatter(\n","                x=list(train_iters)+list(train_iters[::-1]),\n","                y=list(y_upper)+list(y_lower[::-1]),\n","                fill=\"toself\",\n","                fillcolor=color,\n","                line=dict(color=\"rgba(255,255,255,0)\"),\n","                hoverinfo=\"skip\",\n","                showlegend=False,\n","                opacity=0.3\n","            )\n","        )\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=means[i],\n","                name=\"BP\" if i == 0 else \"PC\",\n","                mode=\"lines+markers\",\n","                line=dict(width=3, color=color)\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        xaxis=dict(\n","            title=\"Training iteration\",\n","            tickvals=[1, int(max_train_iter/2), max_train_iter],\n","            ticktext=[1, int(max_train_iter/2), max_train_iter],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\Large{||\\partial \\\\theta||_2}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n"]},{"cell_type":"markdown","metadata":{"id":"2ej8Bs3HUZvn"},"source":["## Scripts"]},{"cell_type":"code","execution_count":76,"metadata":{"cellView":"form","id":"5mhJ2tmHhhDx","executionInfo":{"status":"ok","timestamp":1717439518979,"user_tz":-60,"elapsed":4,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Loss landscape visualisation script\n","\n","\n","def visualise_loss_1mlp_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        weight_updates: List[np.ndarray],\n","        train_losses: List[float],\n","        save_dir: str\n","    ) -> None:\n","    w1s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w2s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","\n","    loss_mesh = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION))\n","    gradient_field = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION, 2))\n","    for k, w1 in enumerate(w1s):\n","        for j, w2 in enumerate(w2s):\n","            loss = ( 0.5 * (y - w2*w1*x )**2 ).mean()\n","            loss_mesh[j, k] = loss\n","\n","            error = y - w2*w1*x\n","            dw1 = ( - error * w2*x ).mean()\n","            dw2 = ( - error * w1*x ).mean()\n","            gradient_field[j, k, 0] = - dw1\n","            gradient_field[j, k, 1] = - dw2\n","\n","\n","    plot_objective_surface(\n","        objective_mesh=loss_mesh,\n","        weights=[w1s, w2s],\n","        save_path=f\"{save_dir}/loss_landscape_surface_{domain}.pdf\",\n","        weight_updates=weight_updates,\n","        train_losses=train_losses\n","    )\n","    plot_objective_contour(\n","        objective_mesh=loss_mesh,\n","        weights=[w1s, w2s],\n","        vector_field=gradient_field,\n","        vector_field_scale=GRADIENT_FIELD_SCALE,\n","        objective_name=\"loss\",\n","        title=\"BP\",\n","        save_path=f\"{save_dir}/loss_landscape_contour_{domain}.pdf\",\n","    )\n","    plot_objective_contour(\n","        objective_mesh=loss_mesh,\n","        weights=[w1s, w2s],\n","        vector_field=gradient_field,\n","        vector_field_scale=GRADIENT_FIELD_SCALE,\n","        objective_name=\"loss\",\n","        title=\"BP\",\n","        save_path=f\"{save_dir}/loss_landscape_contour_{domain}\",\n","        weight_updates=weight_updates\n","    )\n","\n","\n","def visualise_loss_2mlp_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        weight_updates: List[np.ndarray],\n","        save_dir: str\n","    ) -> None:\n","    w1s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w2s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w3s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","\n","    loss_mesh = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION, SAMPLING_RESOLUTION))\n","    for k, w1 in enumerate(w1s):\n","        for j, w2 in enumerate(w2s):\n","            for i, w3 in enumerate(w3s):\n","                loss = ( 0.5 * (y - w3*w2*w1*x )**2 ).mean()\n","                loss_mesh[j, k, i] = loss\n","\n","    plot_objective_volume(\n","        objective_mesh=loss_mesh,\n","        input_domain=domain,\n","        sampling_resolution=SAMPLING_RESOLUTION,\n","        weight_updates=weight_updates,\n","        save_path=f\"{save_dir}/loss_landscape_volume_{domain}\"\n","    )\n","\n","\n","def visualise_loss_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        weight_updates: List[np.ndarray],\n","        train_losses: List[float],\n","        save_dir: str\n","    ):\n","    if len(weight_updates) == 2:\n","        visualise_loss_1mlp_landscape(\n","            domain=domain,\n","            x=x,\n","            y=y,\n","            weight_updates=weight_updates,\n","            train_losses=train_losses,\n","            save_dir=save_dir\n","        )\n","    elif len(weight_updates) == 3:\n","        visualise_loss_2mlp_landscape(\n","            domain=domain,\n","            x=x,\n","            y=y,\n","            weight_updates=weight_updates,\n","            save_dir=save_dir\n","        )\n"]},{"cell_type":"code","execution_count":77,"metadata":{"cellView":"form","id":"zooS72zEIBIb","executionInfo":{"status":"ok","timestamp":1717439518979,"user_tz":-60,"elapsed":4,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Energy landscape visualisation script\n","\n","\n","def visualise_energy_1mlp_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        n_iters: int,\n","        dt: float,\n","        weight_updates: List[np.ndarray],\n","        train_losses: List[float],\n","        save_dir: str\n","    ) -> None:\n","    w1s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w2s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","\n","    energy_mesh = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION, n_iters+1))\n","    gradient_field = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION, n_iters+1, 2))\n","    for k, w1 in enumerate(w1s):\n","        for j, w2 in enumerate(w2s):\n","\n","            z = w1*x\n","            e2 = y - w2*z\n","            e1 = z - w1*x\n","            energy = (0.5 * e1**2 + 0.5 * e2**2).mean()\n","            energy_mesh[j, k, 0] = energy\n","\n","            dw1 = ( - e1*x ).mean()\n","            dw2 = ( - e2*z ).mean()\n","            gradient_field[j, k, 0, 0] = - dw1\n","            gradient_field[j, k, 0, 1] = - dw2\n","\n","            for t in range(1, n_iters+1):\n","                dz = e1 - w2*e2\n","                z += - dz * dt\n","\n","                e2 = (y - w2*z)\n","                e1 = (z - w1*x)\n","                energy = (0.5 * e1**2 + 0.5 * e2**2).mean()\n","                energy_mesh[j, k, t] = energy\n","\n","                dw1 = ( - e1*x ).mean()\n","                dw2 = ( - e2*z ).mean()\n","                gradient_field[j, k, t, 0] = - dw1\n","                gradient_field[j, k, t, 1] = - dw2\n","\n","    # equilibrated energy landscape, w/ updates\n","    plot_objective_contour(\n","        objective_mesh=energy_mesh[:, :, -1],\n","        weights=[w1s, w2s],\n","        vector_field=gradient_field[:, :, -1],\n","        vector_field_scale=GRADIENT_FIELD_SCALE,\n","        objective_name=\"energy\",\n","        title=\"PC\",\n","        save_path=f\"{save_dir}/equilib_energy_landscape_contour_{domain}\",\n","        weight_updates=weight_updates\n","    )\n","\n","    # energy landscape inference dynamics, w/o updates\n","    surface_frames, contour_frames = [], []\n","    for t in range(n_iters+1):\n","        if t in PLOT_INFER_ITERATIONS:\n","\n","            fig = plot_objective_surface(\n","                objective_mesh=energy_mesh[:, :, t],\n","                weights=[w1s, w2s],\n","                save_path=f\"{save_dir}/energy_landscape_surface_{domain}_iter_{t}.pdf\"\n","            )\n","            surface_frames.append(fig)\n","\n","            fig = plot_objective_contour(\n","                objective_mesh=energy_mesh[:, :, t],\n","                weights=[w1s, w2s],\n","                vector_field=gradient_field[:, :, t],\n","                vector_field_scale=GRADIENT_FIELD_SCALE,\n","                objective_name=\"energy\",\n","                title=f\"PC, t = {t}\",\n","                save_path=f\"{save_dir}/energy_landscape_contour_{domain}_iter_{t}.pdf\"\n","            )\n","            contour_frames.append(fig)\n","\n","    plot_objective_surface(\n","        objective_mesh=energy_mesh[:, :, -1],\n","        weights=[w1s, w2s],\n","        save_path=f\"{save_dir}/equilib_energy_landscape_surface_{domain}_updates.pdf\",\n","        weight_updates=weight_updates,\n","        train_losses=train_losses\n","    )\n","\n","    gif.save(\n","        frames=surface_frames,\n","        path=f\"{save_dir}/energy_landscape_surface_{domain}_infer_dynamics.gif\",\n","        duration=1,\n","        unit=\"s\"\n","    )\n","    gif.save(\n","        frames=contour_frames,\n","        path=f\"{save_dir}/energy_landscape_contour_{domain}_infer_dynamics.gif\",\n","        duration=1,\n","        unit=\"s\"\n","    )\n","\n","\n","def visualise_energy_2mlp_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        n_iters: int,\n","        dt: float,\n","        weight_updates: List[np.ndarray],\n","        save_dir: str\n","    ) -> None:\n","    w1s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w2s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","    w3s = np.linspace(-domain, domain, SAMPLING_RESOLUTION)\n","\n","    energy_mesh = np.zeros(\n","        (\n","            SAMPLING_RESOLUTION,\n","            SAMPLING_RESOLUTION,\n","            SAMPLING_RESOLUTION,\n","            N_ITERS+1\n","        )\n","    )\n","    for k, w1 in enumerate(w1s):\n","        for j, w2 in enumerate(w2s):\n","            for i, w3 in enumerate(w3s):\n","\n","                Z1 = w1*x\n","                Z2 = w2*Z1\n","                e3 = y - w3*Z2\n","                e2 = Z2 - w2*Z1\n","                e1 = Z1 - w1*x\n","                energy = (0.5 * e1**2 + 0.5 * e2**2 + 0.5 * e3**2).mean()\n","                energy_mesh[j, k, i, 0] = energy\n","\n","                for t in range(1, N_ITERS+1):\n","                    dZ1 = - e1 + w2*e2\n","                    dZ2 = - e2 + w3*e3\n","                    Z1 += dZ1 * DT\n","                    Z2 += dZ2 * DT\n","\n","                    e3 = y - w3*Z2\n","                    e2 = Z2 - w2*Z1\n","                    e1 = Z1 - w1*x\n","                    energy = (0.5 * e1**2 + 0.5 * e2**2 + 0.5 * e3**2).mean()\n","                    energy_mesh[j, k, i, t] = energy\n","\n","    # equilibrated energy volume, w/ updates\n","    plot_objective_volume(\n","        objective_mesh=energy_mesh[:, :, :, -1],\n","        input_domain=domain,\n","        sampling_resolution=SAMPLING_RESOLUTION,\n","        save_path=f\"{save_dir}/equilib_energy_volume_{domain}\",\n","        weight_updates=weight_updates\n","    )\n","\n","    # energy volume inference dynamics, w/o updates\n","    volume_frames = []\n","    for t in range(N_ITERS+1):\n","        if t in PLOT_INFER_ITERATIONS:\n","\n","            fig = plot_objective_volume(\n","                objective_mesh=energy_mesh[:, :, :, t],\n","                input_domain=domain,\n","                sampling_resolution=SAMPLING_RESOLUTION,\n","                save_path=f\"{save_dir}/energy_landscape_volume_{domain}_iter_{t}.pdf\"\n","            )\n","            volume_frames.append(fig)\n","\n","    gif.save(\n","        frames=volume_frames,\n","        path=f\"{save_dir}/energy_landscape_volume_infer_dynamics_{domain}.gif\",\n","        duration=1000\n","    )\n","\n","\n","def visualise_energy_landscape(\n","        domain: int,\n","        x: np.ndarray,\n","        y: np.ndarray,\n","        n_iters: int,\n","        dt: float,\n","        weight_updates: List[np.ndarray],\n","        train_losses: List[float],\n","        save_dir: str\n","    ) -> None:\n","    if len(weight_updates) == 2:\n","        visualise_energy_1mlp_landscape(\n","            domain=domain,\n","            x=x,\n","            y=y,\n","            n_iters=N_ITERS,\n","            dt=DT,\n","            weight_updates=weight_updates,\n","            train_losses=train_losses,\n","            save_dir=save_dir\n","        )\n","    elif len(weight_updates) == 3:\n","        visualise_energy_2mlp_landscape(\n","            domain=domain,\n","            x=x,\n","            y=y,\n","            n_iters=N_ITERS,\n","            dt=DT,\n","            weight_updates=weight_updates,\n","            save_dir=save_dir\n","        )\n"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"q3BtY8i1Deki","cellView":"form","executionInfo":{"status":"ok","timestamp":1717439518979,"user_tz":-60,"elapsed":4,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title BP training script\n","\n","\n","def train_bp(weights: np.ndarray, save_dir: str):\n","    n_hidden = len(weights)-1\n","    print(f\"------------------------------------------------------\")\n","    print(f\"Starting training of {n_hidden}-MLP with BP...\\n\")\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    if n_hidden == 1:\n","        w1, w2 = weights[0], weights[1]\n","    else:\n","        w1, w2, w3 = weights[0], weights[1], weights[2]\n","\n","    # metrics\n","    ys, ys_hat, train_losses, test_losses = [], [], [], []\n","    w1_updates, w2_updates = [w1], [w2]\n","    dw1s, dw2s = [], []\n","    grad_norms = []\n","    if n_hidden == 2:\n","         w3_updates = [w3]\n","         dw3s = []\n","\n","    max_train_iters = 15 if n_hidden == 1 else 100\n","    # training\n","    for batch in range(max_train_iters):\n","        # data\n","        X, Y = make_gaussian_dataset(\n","            mean=DATA_MEAN,\n","            std=DATA_STD,\n","            size=BATCH_SIZE\n","        )\n","        ys.append(Y)\n","\n","        # loss\n","        y_hat = w2*w1*X if n_hidden == 1 else w3*w2*w1*X\n","        train_loss = ( 0.5 * (Y - y_hat)**2 ).mean()\n","        train_losses.append(train_loss)\n","        ys_hat.append(y_hat)\n","\n","        # weight gradient\n","        error = Y - w2*w1*X if n_hidden == 1 else Y - w3*w2*w1*X\n","        dw1 = ( - error * w2*X ).mean() if n_hidden == 1 else ( - error * w3*w2*X ).mean()\n","        dw2 = ( - error * w1*X ).mean() if n_hidden == 1 else ( - error * w3*w1*X ).mean()\n","        dw1s.append(dw1)\n","        dw2s.append(dw2)\n","        if n_hidden == 2:\n","           dw3 = ( - error * w2*w1*X ).mean()\n","           dw3s.append(dw3)\n","\n","        grad = np.array([[dw1], [dw2]]) if n_hidden == 1 else np.array([[dw1], [dw2], [dw3]])\n","        grad_norms.append(norm(grad))\n","\n","        # weight update\n","        w1 += - LR * (dw1)\n","        w2 += - LR * (dw2)\n","        w1_updates.append(w1)\n","        w2_updates.append(w2)\n","        if n_hidden == 2:\n","            w3 += - LR * (dw3)\n","            w3_updates.append(w3)\n","\n","        # test loss\n","        x = 1\n","        y = -x\n","        test_loss = ( 0.5 * (y - w2*w1*x)**2 ).mean() if n_hidden == 1 else ( 0.5 * (y - w3*w2*w1*x)**2 ).mean()\n","        test_losses.append(test_loss)\n","        print(f\"test loss: {test_loss:.5f}\")\n","\n","    print(f\"\\nTraining stopped at batch {batch+1} with test loss {test_loss:.5f}\\n\")\n","\n","    # plot losses, weight updates & gradient dynamics\n","    plot_losses(\n","        losses={\"train\": train_losses, \"test\": test_losses},\n","        save_path=f\"{save_dir}/losses.pdf\"\n","    )\n","    weight_updates = [w1_updates, w2_updates] if n_hidden == 1 else [w1_updates, w2_updates, w3_updates]\n","    plot_updates(\n","        updates=weight_updates,\n","        update_type=\"weights\",\n","        save_path=f\"{save_dir}/weights.pdf\"\n","    )\n","    gradient_updates = [dw1s, dw2s] if n_hidden == 1 else [dw1s, dw2s, dw3s]\n","    plot_updates(\n","        updates=gradient_updates,\n","        update_type=\"gradient\",\n","        save_path=f\"{save_dir}/gradient.pdf\"\n","    )\n","\n","    # plot prediction dynamics\n","    n_batches = len(ys)\n","    pred_frames = []\n","    for batch in range(n_batches):\n","        fig = plot_predictions(\n","            targets=ys[batch],\n","            predictions=ys_hat[batch],\n","            title=f\"Training iteration {batch+1}\"\n","        )\n","        pred_frames.append(fig)\n","\n","    gif.save(\n","        frames=pred_frames,\n","        path=f\"{save_dir}/prediction_learning_dynamics.gif\",\n","        duration=200\n","    )\n","\n","    # visualise learning dynamics\n","    X, Y = make_gaussian_dataset(\n","        mean=DATA_MEAN,\n","        std=DATA_STD,\n","        size=BATCH_SIZE\n","    )\n","    if N_SEEDS == 1:\n","        visualise_loss_landscape(\n","            domain=1 if n_hidden == 1 else 2,\n","            x=X,\n","            y=Y,\n","            weight_updates=weight_updates,\n","            train_losses=train_losses,\n","            save_dir=save_dir\n","        )\n","    return train_losses, test_losses, grad_norms\n"]},{"cell_type":"code","execution_count":82,"metadata":{"cellView":"form","id":"gDQO743G1scy","executionInfo":{"status":"ok","timestamp":1717439527568,"user_tz":-60,"elapsed":2,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title PC training script\n","\n","\n","def train_pc(weights: np.ndarray, save_dir: str):\n","    n_hidden = len(weights)-1\n","    print(f\"------------------------------------------------------\")\n","    print(f\"Starting training of {n_hidden}-MLP with PC...\\n\")\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    if n_hidden == 1:\n","        w1, w2 = weights[0], weights[1]\n","    else:\n","        w1, w2, w3 = weights[0], weights[1], weights[2]\n","\n","    # learning metrics\n","    ys, ys_hat, energies1, energies2 = [], [], [], []\n","    train_losses, test_losses = [], []\n","    w1_updates, w2_updates = [w1], [w2]\n","    dw1s, dw2s = [], []\n","    gradient_norms = []\n","    if n_hidden == 2:\n","        energies3 = []\n","        w3_updates = [w3]\n","        dw3s = []\n","\n","    # training\n","    for batch in range(50):\n","        # inference metric\n","        ys_hat_iters = np.zeros((BATCH_SIZE, N_ITERS+1))\n","\n","        # data\n","        X, Y = make_gaussian_dataset(\n","            mean=DATA_MEAN,\n","            std=DATA_STD,\n","            size=BATCH_SIZE\n","        )\n","        ys.append(Y)\n","\n","        if n_hidden == 1:\n","            # initialisation\n","            Z = w1*X\n","            e2 = Y - w2*Z\n","            e1 = Z - w1*X\n","            energy = (0.5 * e1**2 + 0.5 * e2**2).mean()\n","\n","            ys_hat_iters[:, 0] = w2*Z\n","            energies1.append((0.5 * e1**2).mean())\n","            energies2.append((0.5 * e2**2).mean())\n","\n","            # iterative inference\n","            for t in range(1, N_ITERS+1):\n","                dZ = e1 - w2*e2\n","                Z += - dZ * DT\n","\n","                e2 = Y - w2*Z\n","                e1 = Z - w1*X\n","                energy = (0.5 * e1**2 + 0.5 * e2**2).mean()\n","\n","                ys_hat_iters[:, t] = w2*Z\n","                energies1.append((0.5 * e1**2).mean())\n","                energies2.append((0.5 * e2**2).mean())\n","\n","            print(f\"z^*: {Z.mean()}\")\n","            print(f\"theory z^*: {( (w1*X + w2*Y)/(1+w2**2) ).mean()}\\n\")\n","\n","        elif n_hidden == 2:\n","            Z1 = w1*X\n","            Z2 = w2*Z1\n","            e3 = Y - w3*Z2\n","            e2 = Z2 - w2*Z1\n","            e1 = Z1 - w1*X\n","            energy = (0.5 * e1**2 + 0.5 * e2**2 + 0.5 * e3**2).mean()\n","\n","            ys_hat_iters[:, 0] = w3*Z2\n","            energies1.append((0.5 * e1**2).mean())\n","            energies2.append((0.5 * e2**2).mean())\n","            energies3.append((0.5 * e3**2).mean())\n","\n","            for t in range(1, N_ITERS+1):\n","                dZ1 = e1 - w2*e2\n","                dZ2 = e2 - w3*e3\n","                Z1 += - dZ1 * DT\n","                Z2 += - dZ2 * DT\n","\n","                e3 = Y - w3*Z2\n","                e2 = Z2 - w2*Z1\n","                e1 = Z1 - w1*X\n","                energy = (0.5 * e1**2 + 0.5 * e2**2 + 0.5 * e3**2).mean()\n","\n","                ys_hat_iters[:, t] = w3*Z2\n","                energies1.append((0.5 * e1**2).mean())\n","                energies2.append((0.5 * e2**2).mean())\n","                energies3.append((0.5 * e3**2).mean())\n","\n","        ys_hat.append(ys_hat_iters)\n","\n","        # weight gradient\n","        dw1 = ( - e1*X ).mean()\n","        dw2 = ( - e2*Z ).mean() if n_hidden == 1 else ( - e2*Z1 ).mean()\n","        dw1s.append(dw1)\n","        dw2s.append(dw2)\n","        if n_hidden == 2:\n","           dw3 = ( - e3*Z2 ).mean()\n","           dw3s.append(dw3)\n","\n","        grad = np.array([[dw1], [dw2]]) if n_hidden == 1 else np.array([[dw1], [dw2], [dw3]])\n","        gradient_norms.append(norm(grad))\n","\n","        # train loss\n","        train_loss = ( 0.5 * (Y - w2*w1*X )**2 ).mean() if n_hidden == 1 else ( 0.5 * (Y - w3*w2*w1*X )**2 ).mean()\n","        train_losses.append(train_loss)\n","\n","        # weight update\n","        w1 += LR * (- dw1)\n","        w2 += LR * (- dw2)\n","        w1_updates.append(w1)\n","        w2_updates.append(w2)\n","        if n_hidden == 2:\n","            w3 += - LR * (dw3)\n","            w3_updates.append(w3)\n","\n","        # test loss\n","        x = 1\n","        y = -x\n","        test_loss = ( 0.5 * (y - w2*w1*x )**2 ) if n_hidden == 1 else ( 0.5 * (y - w3*w2*w1*x )**2 )\n","        test_losses.append(test_loss)\n","\n","        print(f\"test loss: {test_loss:.5f}\")\n","        if n_hidden == 1 and test_loss < 0.001:\n","            break\n","\n","    print(f\"\\nTraining stopped at batch {batch+1} with total energy {energy:.5f} and test loss {test_loss:.5f}\\n\")\n","\n","    # plot losses, energies & updates\n","    plot_losses(\n","        losses={\"train\": train_losses, \"test\": test_losses},\n","        save_path=f\"{save_dir}/losses.pdf\"\n","    )\n","    energies = [energies1, energies2] if n_hidden == 1 else [energies1, energies2, energies3]\n","    plot_energies(\n","        energies=energies,\n","        n_infer_iters=N_ITERS,\n","        save_path=f\"{save_dir}/energies.pdf\"\n","    )\n","    weight_updates = [w1_updates, w2_updates] if n_hidden == 1 else [w1_updates, w2_updates, w3_updates]\n","    plot_updates(\n","        updates=weight_updates,\n","        update_type=\"weights\",\n","        save_path=f\"{save_dir}/weights.pdf\"\n","    )\n","    gradient_updates = [dw1s, dw2s] if n_hidden == 1 else [dw1s, dw2s, dw3s]\n","    plot_updates(\n","        updates=gradient_updates,\n","        update_type=\"gradient\",\n","        save_path=f\"{save_dir}/gradient.pdf\"\n","    )\n","\n","    # plot predictions dynamics\n","    n_batches = len(ys)\n","    pred_frames = []\n","    for batch in range(n_batches):\n","        for t in range(N_ITERS+1):\n","            if t in [0, int(N_ITERS/2), N_ITERS]:\n","                fig = plot_predictions(\n","                    targets=ys[batch],\n","                    predictions=np.array(ys_hat)[batch, :, t],\n","                    title=f\"Training iteration {batch+1}, t = {t}\"\n","                )\n","                pred_frames.append(fig)\n","\n","    gif.save(\n","        frames=pred_frames,\n","        path=f\"{save_dir}/predictions_infer_learn_dynamics.gif\",\n","        duration=200\n","    )\n","\n","    # visualise learning dynamics\n","    X, Y = make_gaussian_dataset(\n","        mean=DATA_MEAN,\n","        std=DATA_STD,\n","        size=BATCH_SIZE\n","    )\n","    if N_SEEDS == 1:\n","        visualise_energy_landscape(\n","            domain=1 if n_hidden == 1 else 2,\n","            x=X,\n","            y=Y,\n","            n_iters=N_ITERS,\n","            dt=DT,\n","            weight_updates=weight_updates,\n","            train_losses=train_losses,\n","            save_dir=save_dir\n","        )\n","    return train_losses, test_losses, gradient_norms\n"]},{"cell_type":"code","execution_count":80,"metadata":{"cellView":"form","id":"jYxvPXLL7hTm","executionInfo":{"status":"ok","timestamp":1717439519459,"user_tz":-60,"elapsed":6,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Hessian script\n","\n","\n","def compute_loss_and_energy_hessian(\n","        weights: np.ndarray,\n","        save_dir: str\n","    ) -> None:\n","    n_hidden = len(weights)-1\n","    print(f\"Calculating loss and energy Hessian for {n_hidden}-MLP at the origin...\\n\")\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    X, Y = make_gaussian_dataset(\n","        mean=DATA_MEAN,\n","        std=DATA_STD,\n","        size=BATCH_SIZE\n","    )\n","    if n_hidden == 1:\n","        w1, w2 = weights[0], weights[1]\n","        activities = np.array([X, w1*X, Y])\n","    elif n_hidden == 2:\n","        w1, w2, w3 = weights[0], weights[1], weights[2]\n","        activities = np.array([X, w1*X, w2*w1*X, Y])\n","    elif n_hidden == 5:\n","        w1, w2, w3, w4, w5, w6 = weights[0], weights[1], weights[2], weights[3], weights[4], weights[5]\n","        activities = np.array([X, w1*X, w2*w1*X, w3*w2*w1*X, w4*w3*w2*w1*X, w5*w4*w3*w2*w1*X, Y])\n","    elif n_hidden == 10:\n","        w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11 = weights[0], weights[1], weights[2], weights[3], weights[4], weights[5], weights[6], weights[7], weights[8], weights[9], weights[10]\n","        activities = np.array([X, w1*X, w2*w1*X, w3*w2*w1*X, w4*w3*w2*w1*X, w5*w4*w3*w2*w1*X, w6*w5*w4*w3*w2*w1*X, w7*w6*w5*w4*w3*w2*w1*X, w8*w7*w6*w5*w4*w3*w2*w1*X, w9*w8*w7*w6*w5*w4*w3*w2*w1*X, w10*w9*w8*w7*w6*w5*w4*w3*w2*w1*X, Y])\n","\n","    loss_hessian = hessian(mse_loss_fun)(\n","        weights,\n","        X,\n","        Y\n","    )\n","    plot_hessian_matrix(\n","        hessian_matrix=loss_hessian,\n","        save_path=f\"{save_dir}/loss_hessian.pdf\"\n","    )\n","    loss_hessian_eigenvals, _ = eigh(loss_hessian)\n","\n","    energy_hessian_frames = []\n","    for t in range(N_ITERS+1):\n","        energy_hessian = hessian(energy_fun)(\n","            weights,\n","            activities,\n","            n_iters=t,\n","            dt=DT\n","        )\n","        fig = plot_hessian_matrix(\n","            hessian_matrix=energy_hessian,\n","            title=f\"Inference iteration = {t}\",\n","            save_path=f\"{save_dir}/energy_hessian_iter_{t}.pdf\"\n","        )\n","        energy_hessian_frames.append(fig)\n","\n","    gif.save(\n","        frames=energy_hessian_frames,\n","        path=f\"{save_dir}/energy_hessian_infer_dynamics.gif\",\n","        duration=1000\n","    )\n","    energy_hessian_eigenvals, _ = eigh(energy_hessian)\n","\n","    theory_energy_hessian_eigenvals = compute_theoretical_energy_eigenvals(\n","        weights,\n","        X,\n","        Y\n","    )\n","\n","    plot_loss_and_energy_hessian_eigenvals(\n","        hessian_eigenvals=[\n","            loss_hessian_eigenvals,\n","            energy_hessian_eigenvals,\n","            theory_energy_hessian_eigenvals\n","        ],\n","        save_path=f\"{save_dir}/hessian_eigenspectrum.pdf\"\n","    )\n"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"Zy8Af7J07eiM","cellView":"form","executionInfo":{"status":"ok","timestamp":1717439519459,"user_tz":-60,"elapsed":5,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}}},"outputs":[],"source":["#@title Main script\n","\n","\n","def main():\n","    for n_hidden in HIDDEN_UNITS:\n","        n_hidden_dir = f\"{RESULTS_DIR}/n_hidden_{n_hidden}\"\n","        os.makedirs(n_hidden_dir, exist_ok=True)\n","\n","        if n_hidden in [1, 2]:\n","            bp_train_losses_all_seeds = [[] for seed in range(N_SEEDS)]\n","            bp_test_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","            pc_train_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","            pc_test_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","\n","            bp_grad_norms_all_seeds = bp_train_losses_all_seeds.copy()\n","            pc_grad_norms_all_seeds = bp_train_losses_all_seeds.copy()\n","\n","        for seed in range(N_SEEDS):\n","            print(f\"\\nSeed {seed+1}/{N_SEEDS}...\\n\")\n","            if N_SEEDS == 1:\n","                seed = 2 if n_hidden == 2 else 1\n","\n","            set_seed(seed)\n","            weights = np.random.normal(loc=0, scale=WEIGHT_SCALE, size=n_hidden+1)\n","\n","            if n_hidden in [1, 2]:\n","                bp_train_losses, bp_test_losses, bp_grad_norms = train_bp(\n","                    weights=weights,\n","                    save_dir=f\"{n_hidden_dir}/{str(seed)}/bp\"\n","                )\n","                pc_train_losses, pc_test_losses, pc_grad_norms = train_pc(\n","                    weights=weights,\n","                    save_dir=f\"{n_hidden_dir}/{str(seed)}/pc\"\n","                )\n","                idx = seed if N_SEEDS > 1 else 0\n","                bp_train_losses_all_seeds[idx] = bp_train_losses\n","                bp_test_losses_all_seeds[idx] = bp_test_losses\n","                pc_train_losses_all_seeds[idx] = pc_train_losses\n","                pc_test_losses_all_seeds[idx] = pc_test_losses\n","\n","                bp_grad_norms_all_seeds[idx] = bp_grad_norms\n","                pc_grad_norms_all_seeds[idx] = pc_grad_norms\n","\n","            zero_weights = np.random.normal(loc=0, scale=0, size=n_hidden+1)\n","            compute_loss_and_energy_hessian(\n","                weights=zero_weights,\n","                save_dir=f\"{n_hidden_dir}/{str(seed)}/hessians\"\n","            )\n","\n","        if n_hidden in [1, 2]:\n","            bp_train_means, bp_train_stds = compute_metric_stats(metric=bp_train_losses_all_seeds)\n","            bp_test_means, bp_test_stds = compute_metric_stats(metric=bp_test_losses_all_seeds)\n","            pc_train_means, pc_train_stds = compute_metric_stats(metric=pc_train_losses_all_seeds)\n","            pc_test_means, pc_test_stds = compute_metric_stats(metric=pc_test_losses_all_seeds)\n","\n","            bp_grad_norm_means, bp_grad_norm_stds = compute_metric_stats(metric=bp_grad_norms_all_seeds)\n","            pc_grad_norm_means, pc_grad_norm_stds = compute_metric_stats(metric=pc_grad_norms_all_seeds)\n","\n","            plot_bp_and_pc_loss_stats(\n","                means=[bp_train_means, pc_train_means],\n","                stds=[bp_train_stds, pc_train_stds],\n","                loss_title=\"$\\LARGE{\\mathcal{L}_{\\\\text{train}}}$\",\n","                save_path=f\"{n_hidden_dir}/train_loss_stats.pdf\"\n","            )\n","            plot_bp_and_pc_loss_stats(\n","                means=[bp_test_means, pc_test_means],\n","                stds=[bp_test_stds, pc_test_stds],\n","                loss_title=\"$\\LARGE{\\mathcal{L}_{\\\\text{test}}}$\",\n","                save_path=f\"{n_hidden_dir}/test_loss_stats.pdf\"\n","            )\n","            plot_bp_vs_pc_grad_norm_stats(\n","                means=[bp_grad_norm_means, pc_grad_norm_means],\n","                stds=[bp_grad_norm_stds, pc_grad_norm_stds],\n","                save_path=f\"{n_hidden_dir}/gradient_norm_stats.pdf\"\n","            )\n"]},{"cell_type":"markdown","metadata":{"id":"hffq2-QJ7zJ5"},"source":["## Run analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mtiqh47Z-3Oq"},"outputs":[],"source":["main()\n","!zip -r linear_chains_results.zip results"]},{"cell_type":"markdown","metadata":{"id":"Ab6xgThJtcaY"},"source":["## Download results to gdrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2691,"status":"ok","timestamp":1716055178607,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"},"user_tz":-60},"id":"ANo1GDv9tCQ4","outputId":"cc5620e1-eea3-427d-9cba-c0aae8da8776"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import shutil\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1716055180356,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"},"user_tz":-60},"id":"cko9TPERtGx3","outputId":"58676e5e-a9f6-416b-a9f2-d9cc47cc932f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/linear_chains_results.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["colab_link = \"/content/linear_chains_results.zip\"\n","gdrive_link = \"/content/drive/MyDrive/\"\n","shutil.copy(colab_link, gdrive_link)"]}],"metadata":{"colab":{"collapsed_sections":["QyrSXqGjORRE","2ej8Bs3HUZvn","Ab6xgThJtcaY"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
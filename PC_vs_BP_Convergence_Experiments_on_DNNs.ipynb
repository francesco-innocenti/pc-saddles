{"cells":[{"cell_type":"markdown","metadata":{"id":"1b9WyJE3D3Il"},"source":["# Convergence Experiments on DNNs\n","\n","This notebook analyses the convergence of the stochastic gradient descent dynamics of deep neural networks (DNNs) trained with backprop and predictive coding when initialised near the origin.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tiGklA_e9Abi"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","executionInfo":{"elapsed":24133,"status":"ok","timestamp":1722804738638,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"XwkMH9mjx_dc"},"outputs":[],"source":["#@title Installations\n","\n","\n","%%capture\n","!sudo apt install nvidia-utils-515\n","!pip install -U kaleido\n","!pip install gif==3.0.0\n"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","executionInfo":{"elapsed":6620,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"tC-fTIbPAlIk"},"outputs":[],"source":["#@title Imports\n","\n","\n","import os\n","import random\n","import subprocess\n","import numpy as np\n","from typing import Tuple, List, Dict, Optional\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import parameters_to_vector\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","from torch.linalg import norm\n","\n","from jax import jacfwd, jacrev\n","from jax.numpy.linalg import eigh\n","\n","import gif\n","import matplotlib.pyplot as plt\n","import plotly.graph_objs as go\n","import plotly.express as px\n","from plotly.colors import hex_to_rgb\n","from plotly.express.colors import sample_colorscale\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","executionInfo":{"elapsed":20,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"8rlbd0rQGqVM"},"outputs":[],"source":["#@title Config\n","\n","\n","DATASETS = [\"MNIST\", \"Fashion-MNIST\", \"CIFAR10\"]\n","N_HIDDEN_WIDTHS = {\n","    #\"toy_gaussian\": [[3, 4]],\n","    \"MNIST\": [[4, 500]],\n","    \"Fashion-MNIST\": [[4, 500]],\n","    \"CIFAR10\": [[4, None]]\n","}\n","ACT_FNS = [\"linear\", \"tanh\", \"relu\"]\n","INIT_TYPES = [\"origin\"]\n","OPTIMISERS = [\"SGD\"]\n","N_SEEDS = 1\n","\n","DATA_DIR = \"data\"\n","RESULTS_DIR = \"results\"\n","\n","FC_INPUT_DIM = 784\n","FC_OUTPUT_DIM = 10\n","\n","# toy dataset\n","DATA_MEAN, DATA_STD = 1., 0.1\n","INPUT_DIM = 3\n","\n","# PC hyperparameters\n","N_ITERS = 50\n","DT = 0.1\n","\n","# optimization hyperparameters\n","LR = 1e-3\n","BATCH_SIZE = 64\n","MAX_EPOCHS = 35\n","LOG_BATCH_EVERY = 100\n","LOSS_TOLERANCE = 0.001\n","\n","# landscape plotting\n","DOMAINS = [2, 1, 5e-1, 1e-1, 5e-2]\n","SAMPLING_RESOLUTION = 30\n","COLORSCALE = \"RdBu_r\"\n","PLOT_ITERS = [0, 1, 2, 3, 4, 5, 10, 20, 50]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","executionInfo":{"elapsed":18,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"ATq_cOK_Ao8t"},"outputs":[],"source":["#@title Utils\n","\n","\n","def setup_experiment(\n","        results_dir,\n","        dataset,\n","        arch_type,\n","        n_hidden,\n","        width,\n","        act_fn,\n","        init_type,\n","        optimiser,\n","        lr\n","    ):\n","    print(\n","f\"\"\"\n","Starting experiment with configuration:\n","\n","  Dataset: {dataset}\n","  Arch type: {arch_type}\n","  N hidden: {n_hidden}\n","  Width: {width}\n","  Act fn: {act_fn}\n","  Init type: {init_type}\n","  Optimiser: {optimiser}\n","  Learning rate: {lr}\n","\"\"\"\n",")\n","    if arch_type == \"fc\":\n","        experiment_dir = os.path.join(\n","            results_dir,\n","            dataset,\n","            arch_type,\n","            f\"n_hidden_{n_hidden}\",\n","            f\"width_{width}\",\n","            act_fn,\n","            f\"{init_type}_init\",\n","            optimiser,\n","            f\"lr_{lr}\"\n","        )\n","    elif arch_type == \"conv\":\n","        experiment_dir = os.path.join(\n","            results_dir,\n","            dataset,\n","            arch_type,\n","            f\"n_hidden_{n_hidden}\",\n","            act_fn,\n","            f\"{init_type}_init\",\n","            optimiser,\n","            f\"lr_{lr}\"\n","        )\n","    return experiment_dir\n","\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","\n","def get_device():\n","    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def get_parameter_scale(dataset, n_hidden, optimiser):\n","    if dataset in \"toy_gaussian\":\n","        parameter_scale = 1e-1\n","\n","    if optimiser == \"SGD\":\n","        parameter_scale = 5e-3\n","\n","    elif optimiser == \"Adam\":\n","        parameter_scale = 1e-4\n","\n","    return parameter_scale\n","\n","\n","def init_weights(module, param_scale):\n","    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n","        nn.init.normal_(module.weight, mean=0., std=param_scale)\n","\n","\n","def get_architecture_type(dataset):\n","    if dataset in [\"toy_gaussian\", \"MNIST\", \"Fashion-MNIST\"]:\n","        return \"fc\"\n","    else:\n","      return \"conv\"\n","\n","\n","def get_optimiser(id, model, lr):\n","    if id == \"SGD\":\n","        optimizer = optim.SGD(params=model.parameters(), lr=lr)\n","    elif id == \"Adam\":\n","        optimizer = optim.Adam(params=model.parameters(), lr=lr)\n","    else:\n","        raise ValueError(\"Invalid optimiser ID\")\n","    return optimizer\n","\n","\n","def get_gradient_vector(model):\n","    grad_vec = []\n","    for param in model.parameters():\n","        grad_vec.append(param.grad.view(-1))\n","    grad_vec = torch.cat(grad_vec)\n","    return grad_vec\n","\n","\n","def get_min_iter(lists):\n","    min_iter = 100000\n","    for i in lists:\n","        if len(i) < min_iter:\n","            min_iter = len(i)\n","    return min_iter\n","\n","\n","def get_min_iter_metrics(metrics):\n","    n_seeds = len(metrics)\n","    min_iter = get_min_iter(lists=metrics)\n","\n","    min_iter_metrics = np.zeros((n_seeds, min_iter))\n","    for seed in range(n_seeds):\n","        min_iter_metrics[seed, :] = metrics[seed][:min_iter]\n","\n","    return min_iter_metrics\n","\n","\n","def compute_metric_stats(metric):\n","    min_iter_metrics = get_min_iter_metrics(metrics=metric)\n","    metric_means = min_iter_metrics.mean(axis=0)\n","    metric_stds = min_iter_metrics.std(axis=0)\n","    return metric_means, metric_stds\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","executionInfo":{"elapsed":18,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"kfz2BdzNAg79"},"outputs":[],"source":["#@title Datasets\n","\n","\n","def get_dataloaders(dataset_id):\n","    train_data = get_dataset(\n","        id=dataset_id,\n","        train=True,\n","        normalise=True\n","    )\n","    test_data = get_dataset(\n","        id=dataset_id,\n","        train=False,\n","        normalise=True\n","    )\n","    train_loader = DataLoader(\n","        dataset=train_data,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        drop_last=True\n","    )\n","    test_loader = DataLoader(\n","        dataset=test_data,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        drop_last=True\n","    )\n","    return train_loader, test_loader\n","\n","\n","def get_dataset(id, train, normalise):\n","    if id == \"toy_gaussian\":\n","        dataset = make_gaussian_dataset(train=train)\n","    elif id == \"MNIST\":\n","        dataset = MNIST(train=train, normalise=normalise)\n","    elif id == \"Fashion-MNIST\":\n","        dataset = FashionMNIST(train=train, normalise=normalise)\n","    elif id == \"CIFAR10\":\n","        dataset = CIFAR10(train=train, normalise=normalise)\n","    elif id == \"TinyImageNet\":\n","        download_tiny_imagenet()\n","        dataset = Tiny_ImageNet(train=train, normalise=normalise)\n","    return dataset\n","\n","\n","def make_gaussian_dataset(train):\n","    input = torch.normal(\n","        mean=DATA_MEAN,\n","        std=DATA_STD if train else 0,\n","        size=(60000, INPUT_DIM)\n","    )\n","    target = -input\n","    return TensorDataset(input, target)\n","\n","\n","class MNIST(datasets.MNIST):\n","    def __init__(self, train, normalise=True, save_dir=DATA_DIR):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.1307), std=(0.3081)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        img = torch.flatten(img)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","class FashionMNIST(datasets.FashionMNIST):\n","    def __init__(self, train, normalise=True, save_dir=DATA_DIR):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.5), std=(0.5)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        img = torch.flatten(img)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","class CIFAR10(datasets.CIFAR10):\n","    def __init__(self, train, normalise=True, save_dir=f\"{DATA_DIR}/CIFAR10\"):\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.4914, 0.4822, 0.4465),\n","                        std=(0.247, 0.243, 0.261)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(save_dir, download=True, train=train, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        label = one_hot(label)\n","        return img, label\n","\n","\n","class Tiny_ImageNet(datasets.ImageFolder):\n","    def __init__(self, train, normalise=True, save_dir=\"tiny-imagenet-200\"):\n","        dataset_type = \"train\" if train else \"test\"\n","        path = os.path.join(save_dir, f\"{dataset_type}\")\n","        if normalise:\n","            transform = transforms.Compose(\n","                [\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(\n","                        mean=(0.485, 0.456, 0.406),\n","                        std=(0.229, 0.224, 0.225)\n","                    )\n","                ]\n","            )\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        super().__init__(path, transform=transform)\n","\n","    def __getitem__(self, index):\n","        img, label = super().__getitem__(index)\n","        label = one_hot(label, n_classes=200)\n","        return img, label\n","\n","\n","def download_tiny_imagenet():\n","    cmd = f\"\"\"\n","          wget http://cs231n.stanford.edu/tiny-imagenet-200.zip; \\\n","          unzip tiny-imagenet-200.zip\n","    \"\"\"\n","    subprocess.run(cmd, shell=True)\n","\n","\n","def one_hot(labels, n_classes=10):\n","    arr = torch.eye(n_classes)\n","    return arr[labels]\n","\n","\n","def accuracy(predictions, truths):\n","    batch_size = predictions.size(0)\n","    correct = 0\n","    for b in range(batch_size):\n","        if torch.argmax(predictions[b, :]) == torch.argmax(truths[b, :]):\n","            correct += 1\n","    return correct / batch_size\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","executionInfo":{"elapsed":18,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"nxZArgOB-jIL"},"outputs":[],"source":["#@title Archs\n","\n","\n","def get_network(arch_type, dataset, act_fn, n_hidden=None, width=None):\n","    if arch_type == \"fc\":\n","        network = get_fc_network(\n","            dataset=dataset,\n","            n_hidden=n_hidden,\n","            width=width,\n","            act_fn=act_fn\n","        )\n","    elif arch_type == \"conv\":\n","        network = get_conv_network(dataset=dataset, act_fn=act_fn)\n","    else:\n","        raise ValueError(\n","            \"Invalid architecture type ID. Options are 'fc' and 'conv'\"\n","        )\n","    return network\n","\n","\n","def get_fc_network(dataset, n_hidden, width, act_fn):\n","    input_dim = INPUT_DIM if dataset == \"toy_gaussian\" else FC_INPUT_DIM\n","    output_dim = INPUT_DIM if dataset == \"toy_gaussian\" else FC_OUTPUT_DIM\n","\n","    layers = []\n","    for n in range(n_hidden):\n","        n_input = input_dim if n == 0 else width\n","        if act_fn == \"linear\":\n","            hidden_layer = nn.Sequential(nn.Linear(n_input, width, bias=False))\n","        elif act_fn == \"tanh\":\n","            hidden_layer = nn.Sequential(\n","                nn.Linear(n_input, width, bias=False),\n","                nn.Tanh()\n","            )\n","        elif act_fn == \"relu\":\n","            hidden_layer = nn.Sequential(\n","                nn.Linear(n_input, width, bias=False),\n","                nn.ReLU(inplace=True)\n","            )\n","        layers.append(hidden_layer)\n","\n","    output_layer = nn.Sequential(nn.Linear(width, output_dim, bias=False))\n","    layers.append(output_layer)\n","    network = nn.Sequential(*layers)\n","    return network\n","\n","\n","def get_conv_network(dataset, act_fn):\n","    conv_network = []\n","    if act_fn == \"linear\":\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # out size = 16/32\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # out size = 8/16\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        if dataset == \"CIFAR10\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(nn.Linear(4*4 * 256, 4096, bias=False)))\n","            conv_network.append(nn.Sequential(nn.Linear(4096, 10, bias=False)))\n","        elif dataset == \"TinyImageNet\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 8\n","                nn.MaxPool2d(kernel_size=2, stride=2)\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(nn.Linear(4*4 * 512, 8192, bias=False)))\n","            conv_network.append(nn.Sequential(nn.Linear(8192, 200, bias=False)))\n","\n","    elif act_fn == \"tanh\":\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # out size = 16/32\n","            nn.Tanh(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # out size = 8/16\n","            nn.Tanh(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        if dataset == \"CIFAR10\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.Tanh(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Linear(4*4 * 256, 4096, bias=False),\n","                nn.Tanh()\n","            ))\n","            conv_network.append(nn.Linear(4096, 10, bias=False))\n","\n","        elif dataset == \"TinyImageNet\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 8\n","                nn.Tanh(),\n","                nn.MaxPool2d(kernel_size=2, stride=2)\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.Tanh(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Linear(4*4 * 512, 8192, bias=False),\n","                nn.Tanh()\n","            ))\n","            conv_network.append(nn.Linear(8192, 200, bias=False))\n","\n","    elif act_fn == \"relu\":\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # out size = 16/32\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        conv_network.append(nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # out size = 8/16\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        ))\n","        if dataset == \"CIFAR10\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Linear(4*4 * 256, 4096, bias=False),\n","                nn.ReLU(inplace=True)\n","            ))\n","            conv_network.append(nn.Linear(4096, 10, bias=False))\n","\n","        elif dataset == \"TinyImageNet\":\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # out size = 8\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2)\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # out size = 4\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Flatten()\n","            ))\n","            conv_network.append(nn.Sequential(\n","                nn.Linear(4*4 * 512, 8192, bias=False),\n","                nn.ReLU(inplace=True),\n","            ))\n","            conv_network.append(nn.Linear(8192, 200, bias=False))\n","\n","    return nn.Sequential(*conv_network)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","executionInfo":{"elapsed":18,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"IxZmNRPguIue"},"outputs":[],"source":["#@title Models\n","\n","\n","class BPN(nn.Module):\n","    def __init__(self, network):\n","        super(BPN, self).__init__()\n","        self.network = network\n","\n","    def forward(self, x):\n","        x = self.network(x)\n","        return x\n","\n","    def get_weights(self):\n","        weights = []\n","        for param in self.network.parameters():\n","            if len(param.shape) > 1:\n","                weights.append(param.cpu().detach().numpy())\n","        return weights\n","\n","\n","\n","class PCN(object):\n","    def __init__(self, network, dt, device=\"cpu\"):\n","        self.network = network.to(device)\n","        self.n_layers = len(self.network)\n","        self.n_nodes = self.n_layers + 1\n","        self.dt = dt\n","        self.n_params = sum(\n","            p.numel() for p in network.parameters() if p.requires_grad\n","        )\n","        self.device = device\n","\n","    def reset(self):\n","        self.zero_grad()\n","        self.preds = [None] * self.n_nodes\n","        self.errs = [None] * self.n_nodes\n","        self.xs = [None] * self.n_nodes\n","\n","    def reset_xs(self, prior, init_std):\n","        self.set_prior(prior)\n","        self.propagate_xs()\n","        for l in range(self.n_layers):\n","            self.xs[l] = torch.empty(self.xs[l].shape).normal_(\n","                mean=0,\n","                std=init_std\n","            ).to(self.device)\n","\n","    def set_obs(self, obs):\n","        self.xs[-1] = obs.clone()\n","\n","    def set_prior(self, prior):\n","        self.xs[0] = prior.clone()\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","    def propagate_xs(self):\n","        for l in range(1, self.n_layers):\n","            self.xs[l] = self.network[l - 1](self.xs[l - 1])\n","\n","    def infer_train(\n","            self,\n","            obs,\n","            prior,\n","            n_iters,\n","            record_grad_norms=False,\n","        ):\n","        self.reset()\n","        self.set_prior(prior)\n","        self.propagate_xs()\n","        self.set_obs(obs)\n","\n","        if record_grad_norms:\n","            grad_norms_iters = [np.zeros(n_iters) for p in range(len(self.network)*2)]\n","\n","        dt = self.dt\n","        tot_energy_iters = []\n","        for t in range(n_iters):\n","            self.network.zero_grad()\n","            self.preds[-1] = self.network[self.n_layers - 1](self.xs[self.n_layers - 1])\n","            self.errs[-1] = self.xs[-1] - self.preds[-1]\n","\n","            for l in reversed(range(1, self.n_layers)):\n","                self.preds[l] = self.network[l - 1](self.xs[l - 1])\n","                self.errs[l] = self.xs[l] - self.preds[l]\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[l],\n","                    self.xs[l],\n","                    self.errs[l + 1]\n","                )\n","                with torch.no_grad():\n","                    dx = epsdfdx - self.errs[l]\n","                    self.xs[l] = self.xs[l] + self.dt * dx\n","\n","            tot_energy_iters.append(self.compute_tot_energy())\n","            if t > 0 and tot_energy_iters[t] >= tot_energy_iters[t-1] and self.dt > 0.025:\n","                self.dt /= 2\n","                if self.dt <= 0.025:\n","                    self.dt = dt\n","                    break\n","\n","            if (t+1) != n_iters:\n","                self.clear_grads()\n","\n","        if record_grad_norms:\n","            self.set_grads(\n","                grad_norms_iters=grad_norms_iters,\n","                t=t\n","            )\n","        else:\n","            self.set_grads()\n","\n","        if record_grad_norms:\n","            return tot_energy_iters, grad_norms_iters\n","        else:\n","            return tot_energy_iters\n","\n","    def infer_test(\n","            self,\n","            obs,\n","            prior,\n","            n_iters,\n","            update_prior=True,\n","            update_obs=False,\n","            init_std=0.05,\n","        ):\n","\n","        self.reset()\n","        self.reset_xs(prior, init_std)\n","        if not update_prior:\n","            self.set_prior(prior)\n","        self.set_obs(obs)\n","\n","        for t in range(n_iters):\n","            self.network.zero_grad()\n","            self.preds[-1] = self.network[self.n_layers - 1](self.xs[self.n_layers - 1])\n","            self.errs[-1] = self.xs[-1] - self.preds[-1]\n","\n","            if update_obs:\n","                with torch.no_grad():\n","                    self.xs[-1] = self.xs[-1] + self.dt * (- self.errs[-1])\n","\n","            for l in reversed(range(1, self.n_layers)):\n","                self.preds[l] = self.network[l - 1](self.xs[l - 1])\n","                self.errs[l] = self.xs[l] - self.preds[l]\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[l],\n","                    self.xs[l],\n","                    self.errs[l + 1]\n","                )\n","                with torch.no_grad():\n","                    dx = epsdfdx - self.errs[l]\n","                    self.xs[l] = self.xs[l] + self.dt * dx\n","\n","            if update_prior:\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[0],\n","                    self.xs[0],\n","                    self.errs[1]\n","                )\n","                with torch.no_grad():\n","                    self.xs[0] = self.xs[0] + self.dt * epsdfdx\n","\n","            if (t+1) != n_iters:\n","                self.clear_grads()\n","\n","        if update_prior:\n","            return self.xs[0]\n","        elif update_obs:\n","            return self.xs[-1]\n","\n","    def set_grads(self, grad_norms_iters=None, t=None):\n","        n = 0\n","        for l in range(self.n_layers):\n","            for i, param in enumerate(self.network[l].parameters()):\n","                dparam = torch.autograd.grad(\n","                    self.preds[l + 1],\n","                    param,\n","                    - self.errs[l + 1],\n","                    allow_unused=True,\n","                    retain_graph=True\n","                )[0]\n","                param.grad = dparam.clone()\n","                if grad_norms_iters is not None:\n","                    grad_norm = torch.linalg.norm(dparam)\n","                    grad_norms_iters[n][t] = grad_norm.item()\n","                    n += 1\n","\n","    def zero_grad(self):\n","        self.network.zero_grad()\n","\n","    def clear_grads(self):\n","        with torch.no_grad():\n","            for l in range(1, self.n_nodes):\n","                self.preds[l] = self.preds[l].clone()\n","                self.errs[l] = self.errs[l].clone()\n","                self.xs[l] = self.xs[l].clone()\n","\n","    def save_weights(self, path):\n","        torch.save(self.network.state_dict(), path)\n","\n","    def load_weights(self, path):\n","        self.network.load_state_dict(torch.load(path))\n","\n","    def get_weights(self):\n","        weights = []\n","        for param in self.network.parameters():\n","            if len(param.shape) > 1:\n","                weights.append(param.cpu().detach().numpy())\n","        return weights\n","\n","    def compute_tot_energy(self):\n","        energy = 0.\n","        for err in self.errs:\n","            if err is not None:\n","                energy += (err**2).sum()\n","        return energy.item()\n","\n","    def parameters(self):\n","        return self.network.parameters()\n","\n","    def __str__(self):\n","        return f\"PCN(\\n{self.network}\\n\"\n"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","executionInfo":{"elapsed":17,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"9KS3wxwvDNdB"},"outputs":[],"source":["#@title Weight manipulation\n","# Adapted from https://github.com/tomgoldstein/loss-landscape/blob/master/net_plotter.py\n","\n","\n","def get_weights(net):\n","    \"\"\" Extract parameters from net, and return a list of tensors\"\"\"\n","    return [p.data for p in net.parameters()]\n","\n","\n","def get_random_weights(weights, device='cpu'):\n","    \"\"\"\n","        Produce a random direction that is a list of random Gaussian tensors\n","        with the same shape as the network's weights, so one direction entry per weight.\n","    \"\"\"\n","    return [torch.randn(w.size()).to(device) for w in weights]\n","\n","\n","def normalize_direction(direction, weights, norm='filter'):\n","    \"\"\"\n","        Rescale the direction so that it has similar norm as their corresponding\n","        model in different levels.\n","\n","        Args:\n","          direction: a variables of the random direction for one layer\n","          weights: a variable of the original model for one layer\n","          norm: normalization method, 'filter' | 'layer' | 'weight'\n","    \"\"\"\n","    if norm == 'filter':\n","        # Rescale the filters (weights in group) in 'direction' so that each\n","        # filter has the same norm as its corresponding filter in 'weights'.\n","        for d, w in zip(direction, weights):\n","            d.mul_(w.norm()/(d.norm() + 1e-10))\n","    elif norm == 'layer':\n","        # Rescale the layer variables in the direction so that each layer has\n","        # the same norm as the layer variables in weights.\n","        direction.mul_(weights.norm()/direction.norm())\n","    elif norm == 'weight':\n","        # Rescale the entries in the direction so that each entry has the same\n","        # scale as the corresponding weight.\n","        direction.mul_(weights)\n","    elif norm == 'dfilter':\n","        # Rescale the entries in the direction so that each filter direction\n","        # has the unit norm.\n","        for d in direction:\n","            d.div_(d.norm() + 1e-10)\n","    elif norm == 'dlayer':\n","        # Rescale the entries in the direction so that each layer direction has\n","        # the unit norm.\n","        direction.div_(direction.norm())\n","\n","\n","def normalize_directions_for_weights(direction, weights, norm='filter', ignore='biasbn'):\n","    \"\"\"\n","        The normalization scales the direction entries according to the entries of weights.\n","    \"\"\"\n","    assert(len(direction) == len(weights))\n","    for d, w in zip(direction, weights):\n","        if d.dim() <= 1:\n","            if ignore == 'biasbn':\n","                d.fill_(0) # ignore directions for weights with 1 dimension\n","            else:\n","                d.copy_(w) # keep directions for weights/bias that are only 1 per node\n","        else:\n","            normalize_direction(d, w, norm)\n","\n","\n","def create_random_weight_direction(net, device='cpu', ignore='biasbn', norm='filter'):\n","    \"\"\"\n","        Setup a random (normalized) direction with the same dimension as\n","        the weights.\n","        Args:\n","          net: the given trained model\n","          ignore: 'biasbn', ignore biases and BN parameters.\n","          norm: direction normalization method, including\n","                'filter\" | 'layer' | 'weight' | 'dlayer' | 'dfilter'\n","        Returns:\n","          direction: a random direction with the same dimension as weights.\n","    \"\"\"\n","\n","    # random direction\n","    weights = get_weights(net) # a list of parameters.\n","    direction = get_random_weights(weights, device)\n","    normalize_directions_for_weights(direction, weights, norm, ignore)\n","    return direction\n"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","executionInfo":{"elapsed":17,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"eMJJ1NyknHyU"},"outputs":[],"source":["#@title Plotting\n","\n","\n","@gif.frame\n","def plot_hessian_matrix(hessian_matrix, save_path, title=None):\n","    fig, ax = plt.subplots()\n","    heatmap = ax.imshow(\n","        X=hessian_matrix,\n","        cmap=\"RdBu_r\",\n","        vmin=-1,\n","        vmax=1\n","    )\n","    fig.colorbar(heatmap, ax=ax, location=\"right\")\n","    if title is not None:\n","        plt.title(f\"${title}$\", fontsize=18)\n","    fig.savefig(save_path)\n","    return fig\n","\n","\n","@gif.frame\n","def plot_hessian_eigenvalues(eigenvalues, save_path, title=None):\n","    fig = go.Figure(\n","        data=go.Histogram(\n","            x=eigenvalues,\n","            histnorm=\"probability\",\n","            marker_color=\"#FF7F0E\"\n","        )\n","    )\n","    fig.update_layout(\n","        height=300,\n","        width=500,\n","        title=dict(\n","            text=f\"${title}$\" if title is not None else \"\",\n","            y=0.7,\n","            x=0.5,\n","            xanchor=\"center\",\n","            yanchor=\"top\"\n","        ),\n","        xaxis=dict(title=\"Hessian eigenvalue\"),\n","        yaxis=dict(\n","            title=f\"Density (Log Scale)\",\n","            nticks=5,\n","            type=\"log\"\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","    return fig\n","\n","\n","def plot_loss(loss, mode, save_path):\n","    n_train_iters = len(loss)\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    loss_color = \"#EF553B\"\n","    fig = go.Figure()\n","    fig.add_trace(\n","        go.Scatter(\n","            x=train_iters,\n","            y=loss,\n","            mode=\"lines\",\n","            line=dict(width=2, color=loss_color),\n","            showlegend=False\n","        )\n","    )\n","    fig.update_layout(\n","        height=300,\n","        width=400,\n","        xaxis=dict(\n","            title=\"Batch\" if mode == \"train\" else \"Epoch\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]]\n","        ),\n","        yaxis=dict(\n","            title=f\"$\\Large{{\\mathcal{{L}}_{{{mode}}}}}$\"\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_loss_and_accuracy(loss, accuracy, mode, save_path):\n","    n_train_iters = len(loss)\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    loss_color, accuracy_color = \"#EF553B\", \"#636EFA\"\n","    fig = go.Figure()\n","    fig.add_trace(\n","        go.Scatter(\n","            x=train_iters,\n","            y=loss,\n","            mode=\"lines\",\n","            line=dict(width=2, color=loss_color),\n","            showlegend=False\n","        )\n","    )\n","    fig.add_trace(\n","        go.Scatter(\n","            x=train_iters,\n","            y=accuracy,\n","            mode=\"lines\",\n","            line=dict(width=2, color=accuracy_color),\n","            showlegend=False,\n","            yaxis=\"y2\"\n","        )\n","    )\n","    fig.update_layout(\n","        height=300,\n","        width=400,\n","        xaxis=dict(\n","            title=\"Batch\" if mode == \"train\" else \"Epoch\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]]\n","        ),\n","        yaxis=dict(\n","            title=f\"$\\Large{{\\mathcal{{L}}_{{{mode}}}}}$\",\n","            titlefont=dict(\n","                color=loss_color\n","            ),\n","            tickfont=dict(\n","                color=loss_color\n","            )\n","        ),\n","        yaxis2=dict(\n","            title=f\"{mode.capitalize()} accuracy (%)\",\n","            side=\"right\",\n","            overlaying=\"y\",\n","            titlefont=dict(\n","                color=accuracy_color\n","            ),\n","            tickfont=dict(\n","                color=accuracy_color,\n","            )\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_norms(norms, norm_type, mode, save_path):\n","    n_params = len(norms)\n","    n_iterations = len(norms[0])\n","    iterations = [b+1 for b in range(n_iterations)]\n","\n","    fig = go.Figure()\n","    weights_id = [\n","        f\"$W_{i+1}$\" if norm_type == \"parameters\" else f\"$\\partial W_{i+1}$\" for i in range(n_params)\n","    ]\n","    colors = px.colors.qualitative.Plotly[2:]\n","    for weight_norms, weight_id, color in zip(norms, weights_id, colors):\n","        fig.add_traces(\n","            go.Scatter(\n","                x=iterations,\n","                y=weight_norms,\n","                name=weight_id,\n","                mode=\"lines\",\n","                line=dict(width=2, color=color)\n","            )\n","        )\n","\n","    fig_width = 300 if norm_type == \"parameters\" else 400\n","    xaxis_title = \"Training iteration\" if mode == \"learning\" else \"Inference iteration\"\n","    fig.update_layout(\n","        height=300,\n","        width=fig_width,\n","        xaxis=dict(\n","            title=xaxis_title,\n","            tickvals=[1, int(iterations[-1]/2), iterations[-1]],\n","            ticktext=[1, int(iterations[-1]/2), iterations[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\Large{||W||_F}$\" if norm_type == \"parameters\" else \"$\\Large{||\\partial W||_F}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(f\"{save_path}_weight.pdf\")\n","\n","\n","def plot_bp_and_pc_metric_stats(means, stds, dataset, optimiser, metric_title, save_path):\n","    max_train_iter = len(means[0]) if len(means[0]) >= len(means[1]) else len(means[1])\n","\n","    fig = go.Figure()\n","    for i in range(2):\n","        n_train_iters = len(means[i])\n","        train_iters = [b+1 for b in range(n_train_iters)]\n","\n","        color = \"#EF553B\" if i == 0 else \"#636EFA\"\n","        y_upper, y_lower = means[i] + stds[i], means[i] - stds[i]\n","\n","        fig.add_traces(\n","            go.Scatter(\n","                x=list(train_iters)+list(train_iters[::-1]),\n","                y=list(y_upper)+list(y_lower[::-1]),\n","                fill=\"toself\",\n","                fillcolor=color,\n","                line=dict(color=\"rgba(255,255,255,0)\"),\n","                hoverinfo=\"skip\",\n","                showlegend=False,\n","                opacity=0.3\n","            )\n","        )\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=means[i],\n","                name=\"BP\" if i == 0 else \"PC\",\n","                mode=\"lines+markers\",\n","                line=dict(width=3, color=color)\n","            )\n","        )\n","\n","    if \"train\" in metric_title:\n","        xaxis_title = \"Training iteration (log)\" if dataset == \"toy_gaussian\" else \"Training iteration\"\n","    else:\n","        xaxis_title = \"Epoch\"\n","\n","    if dataset == \"toy_gaussian\":\n","        fig.update_layout(\n","            xaxis=dict(\n","                type=\"log\",\n","                exponentformat=\"power\",\n","                dtick=1\n","            )\n","        )\n","    else:\n","        fig.update_layout(\n","            xaxis=dict(\n","                tickvals=[1, int(max_train_iter/2), max_train_iter],\n","                ticktext=[1, int(max_train_iter/2), max_train_iter]\n","            )\n","        )\n","    if optimiser == \"Adam\":\n","        fig.update_yaxes(range=[0, 0.15])\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        xaxis=dict(title=xaxis_title),\n","        yaxis=dict(title=metric_title),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_bp_vs_pc_grad_norm_stats(\n","        means: Tuple[np.ndarray],\n","        stds: Tuple[np.ndarray],\n","        dataset: int,\n","        save_path: str\n","    ) -> None:\n","    max_train_iter = len(means[0]) if len(means[0]) >= len(means[1]) else len(means[1])\n","\n","    fig = go.Figure()\n","    for i in range(2):\n","        n_train_iters = len(means[i])\n","        train_iters = [b+1 for b in range(n_train_iters)]\n","\n","        color = \"#EF553B\" if i == 0 else \"#636EFA\"\n","        y_upper, y_lower = means[i] + stds[i], means[i] - stds[i]\n","\n","        fig.add_traces(\n","            go.Scatter(\n","                x=list(train_iters)+list(train_iters[::-1]),\n","                y=list(y_upper)+list(y_lower[::-1]),\n","                fill=\"toself\",\n","                fillcolor=color,\n","                line=dict(color=\"rgba(255,255,255,0)\"),\n","                hoverinfo=\"skip\",\n","                showlegend=False,\n","                opacity=0.3\n","            )\n","        )\n","        fig.add_traces(\n","            go.Scatter(\n","                x=train_iters,\n","                y=means[i],\n","                name=\"BP\" if i == 0 else \"PC\",\n","                mode=\"lines+markers\",\n","                line=dict(width=3, color=color)\n","            )\n","        )\n","\n","    if dataset == \"toy_gaussian\":\n","        fig.update_layout(\n","            xaxis=dict(\n","                type=\"log\",\n","                exponentformat=\"power\",\n","                dtick=1\n","            )\n","        )\n","    else:\n","        fig.update_layout(\n","            xaxis=dict(\n","                tickvals=[1, int(max_train_iter/2), max_train_iter],\n","                ticktext=[1, int(max_train_iter/2), max_train_iter]\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        xaxis=dict(\n","            title=\"Training iteration (log)\" if dataset == \"toy_gaussian\" else \"Training iteration\"\n","        ),\n","        yaxis=dict(\n","            title=\"$\\Large{||\\partial \\\\theta||_2}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(save_path)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","executionInfo":{"elapsed":16,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"zCfc_18QnDCv"},"outputs":[],"source":["#@title Landscape plotting\n","\n","\n","@gif.frame\n","def plot_objective_surface(\n","        objective_mesh: np.ndarray,\n","        weights: Tuple[np.ndarray, np.ndarray],\n","        objective_name: str,\n","        save_path: str,\n","        inference_step: Optional[int] = None,\n","        show_background: bool = True\n","    ) -> go.Figure():\n","    objective_notation = \"L\" if objective_name == \"loss\" else \"F\"\n","    objective_max, objective_min = objective_mesh.max(), objective_mesh.min()\n","    fig = go.Figure(\n","        data=go.Surface(\n","            z=objective_mesh,\n","            x=weights[0],\n","            y=weights[1],\n","            colorscale=COLORSCALE,\n","            colorbar=dict(\n","                title=f\"$\\LARGE{{\\mathcal{{{objective_notation}}}}}$\",\n","                x=0.85,\n","                y=0.57,\n","                len=0.3,\n","                titleside=\"right\",\n","                tickfont=dict(size=16),\n","                tickvals=[objective_min, objective_max],\n","                ticktext=[\"Low\", \"High\"]\n","            )\n","        )\n","    )\n","    fig.update_traces(\n","        contours_z=dict(\n","            show=True,\n","            usecolormap=True,\n","            highlightcolor=\"limegreen\",\n","            project_z=True,\n","        )\n","    )\n","    fig.update_layout(\n","        scene=dict(zaxis=(dict(\n","            title=\"\",\n","            range=[objective_min, objective_max],\n","            showticklabels=False\n","        ))),\n","        font=dict(size=16),\n","        height=600,\n","        width=700,\n","        margin=dict(r=30, b=10, l=0, t=40),\n","        scene_aspectmode=\"cube\"\n","    )\n","    if show_background:\n","        title = f\"Inference step: {inference_step}\" if inference_step is not None else \"\"\n","        fig.update_layout(\n","            scene=dict(\n","                xaxis=dict(title=\"\", nticks=3, autorange=\"reversed\"),\n","                yaxis=dict(title=\"\", nticks=3, autorange=\"reversed\")\n","            ),\n","            scene_camera=dict(\n","                center=dict(x=0.05, y=0.2, z=0),\n","                eye=dict(x=1.4, y=1.4, z=1.25)\n","            ),\n","            title=dict(\n","                text=title,\n","                y=0.95,\n","                x=0.42,\n","                xanchor=\"center\",\n","                yanchor=\"top\"\n","            )\n","        )\n","    else:\n","        fig.update_layout(\n","            scene=dict(\n","                xaxis=dict(\n","                    title=\"\",\n","                    autorange=\"reversed\",\n","                    showticklabels=False,\n","                    showbackground=False,\n","                ),\n","                yaxis=dict(\n","                    title=\"\",\n","                    autorange=\"reversed\",\n","                    showticklabels=False,\n","                    showbackground=False,\n","                )\n","            ),\n","            scene_camera=dict(\n","                center=dict(x=0.2, y=0.15, z=0),\n","                eye=dict(x=1.3, y=1.3, z=1)\n","            )\n","        )\n","        fig.update_traces(showscale=False)\n","\n","    fig.write_image(save_path)\n","    return fig\n","\n","\n","@gif.frame\n","def plot_objective_contour(\n","        objective_mesh: np.ndarray,\n","        weights: Tuple[np.ndarray, np.ndarray],\n","        objective_name: str,\n","        save_path: str,\n","        inference_step: Optional[int] = None,\n","        show_origin: bool = True\n","    ):\n","    fig = go.Figure(\n","        data=go.Contour(\n","            z=objective_mesh,\n","            x=weights[0],\n","            y=weights[1],\n","            colorscale=COLORSCALE,\n","            showscale=False,\n","            contours_coloring=\"heatmap\",\n","            # contours=dict(\n","            #     showlabels=True,\n","            #     labelfont=dict(\n","            #         size=12,\n","            #         color=\"black\"\n","            #     )\n","            # )\n","        )\n","    )\n","    # colorbar\n","    objective_min, objective_max = 0, objective_mesh.max()\n","    objective_notation = \"L\" if objective_name == \"loss\" else \"F\"\n","    colorbar = go.Scatter(\n","        x=[None],\n","        y=[None],\n","        mode=\"markers\",\n","        showlegend=False,\n","        marker=dict(\n","            colorscale=COLORSCALE,\n","            showscale=True,\n","            cmin=objective_min,\n","            cmax=objective_max,\n","            colorbar=dict(\n","                title=f\"$\\LARGE{{\\mathcal{{{objective_notation}}}}}$\",\n","                x=1.05,\n","                len=0.5,\n","                title_side=\"right\",\n","                tickfont=dict(size=16),\n","                tickvals=[objective_min, objective_max],\n","                ticktext=[\"Low\", \"High\"]\n","            )\n","        ),\n","        hoverinfo=\"none\"\n","    )\n","    fig.add_trace(colorbar)\n","\n","    if show_origin:\n","        marker_color = \"rgb(255, 255, 51)\"\n","        fig.add_traces(\n","            go.Scatter(\n","                x=[0],\n","                y=[0],\n","                mode=\"markers+text\",\n","                marker=dict(size=10, color=marker_color),\n","                showlegend=False,\n","                text=[f\"$\\LARGE{{\\\\theta^*}}$\"],\n","                textposition=\"top right\",\n","                textfont=dict(color=marker_color)\n","            )\n","        )\n","\n","    title = f\"Inference step: {inference_step}\" if inference_step is not None else \"\"\n","    fig.update_layout(\n","        title=dict(\n","            text=title,\n","            y=0.85,\n","            x=0.41,\n","            xanchor=\"center\",\n","            yanchor=\"top\"\n","        ),\n","        xaxis=dict(title=\"$\\LARGE{α}$\"),\n","        yaxis=dict(title=\"$\\LARGE{β}$\"),\n","        margin=dict(r=200, b=20, l=20, t=100),\n","        font=dict(size=16),\n","        plot_bgcolor=\"white\",\n","        width=700,\n","        height=400\n","    )\n","\n","    fig.write_image(save_path)\n","    return fig\n"]},{"cell_type":"code","execution_count":11,"metadata":{"cellView":"form","executionInfo":{"elapsed":16,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"1AeLLiT7kg6d"},"outputs":[],"source":["#@title Projection visualisations\n","\n","\n","def visualise_2D_loss_random_projections(\n","        model,\n","        input,\n","        target,\n","        domain,\n","        device,\n","        save_dir\n","    ):\n","    n_directions = 2\n","    random_directions = []\n","    for i in range(n_directions):\n","        random_direction = create_random_weight_direction(\n","            net=model.network,\n","            device=device\n","        )\n","        random_direction = [w for w in random_direction if len(w.shape) > 1]\n","        random_directions.append(random_direction)\n","\n","    scaling_factors = [\n","        np.linspace(\n","            -domain, domain, SAMPLING_RESOLUTION\n","        ) for d in range(n_directions)\n","    ]\n","\n","    loss_fn = nn.MSELoss()\n","    loss_mesh = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION))\n","    for j, a in enumerate(scaling_factors[0]):\n","        for i, b in enumerate(scaling_factors[1]):\n","\n","            n = 0\n","            for p in model.parameters():\n","                if len(p.shape) > 1:\n","                    p.data = p.data + (a * random_directions[0][n]) + (b * random_directions[1][n])\n","                    n += 1\n","\n","            preds = model.forward(input)\n","            loss = loss_fn(preds, target)\n","            loss_mesh[i, j] = loss\n","\n","    surface_fig = plot_objective_surface(\n","        objective_mesh=loss_mesh,\n","        weights=scaling_factors,\n","        objective_name=\"loss\",\n","        save_path=f\"{save_dir}/random_surface_{domain}.pdf\"\n","    )\n","\n","\n","def visualise_2D_energy_random_projections(\n","        model,\n","        input,\n","        target,\n","        domain,\n","        device,\n","        save_dir\n","    ):\n","    n_directions = 2\n","    random_directions = []\n","    for i in range(n_directions):\n","        random_direction = create_random_weight_direction(\n","            net=model.network,\n","            device=device\n","        )\n","        random_direction = [w for w in random_direction if len(w.shape) > 1]\n","        random_directions.append(random_direction)\n","\n","    scaling_factors = [\n","        np.linspace(\n","            -domain, domain, SAMPLING_RESOLUTION\n","        ) for d in range(n_directions)\n","    ]\n","\n","    loss_fn = nn.MSELoss()\n","    energy_mesh = np.zeros((SAMPLING_RESOLUTION, SAMPLING_RESOLUTION, N_ITERS+1))\n","    for j, a in enumerate(scaling_factors[0]):\n","        for i, b in enumerate(scaling_factors[1]):\n","\n","            n = 0\n","            for p in model.parameters():\n","                if len(p.shape) > 1:\n","                    p.data = p.data + (a * random_directions[0][n]) + (b * random_directions[1][n])\n","                    n += 1\n","\n","            preds = model.forward(input)\n","            loss = loss_fn(preds, target)\n","            energy_mesh[i, j, 0] = loss\n","\n","            tot_energy_iters = model.infer_train(\n","                obs=target,\n","                prior=input,\n","                n_iters=N_ITERS\n","            )\n","            if len(tot_energy_iters) != N_ITERS:\n","                n_missing_iters = N_ITERS - len(tot_energy_iters)\n","                tot_energy_iters.extend([tot_energy_iters[-1]]*n_missing_iters)\n","\n","            energy_mesh[i, j, 1:] = tot_energy_iters\n","\n","    energy_surface_frames = []\n","    energy_iters_max = energy_mesh.max()\n","    for t in range(N_ITERS+1):\n","        if t in PLOT_ITERS:\n","            fig = plot_objective_surface(\n","                objective_mesh=energy_mesh[:, :, t],\n","                weights=scaling_factors,\n","                objective_name=\"energy\",\n","                inference_step=t,\n","                save_path=f\"{save_dir}/random_surface_{domain}_iter_{t}.pdf\"\n","            )\n","            energy_surface_frames.append(fig)\n","\n","        gif.save(\n","            energy_surface_frames,\n","            f\"{save_dir}/random_surface_{domain}_dynamics.gif\",\n","            duration=1,\n","            unit=\"s\"\n","        )\n"]},{"cell_type":"markdown","metadata":{"id":"U3Td0mTf9C3q"},"source":["## Scripts"]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","executionInfo":{"elapsed":16,"status":"ok","timestamp":1722804745255,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"_TJQ4rwVWnTi"},"outputs":[],"source":["#@title BP train script\n","\n","\n","def train_bp(dataset, arch_type, n_hidden, width, act_fn, init_type, optimiser, seed, max_epochs, save_dir):\n","    print(\"\\nStarting training with BP...\\n\")\n","    set_seed(seed)\n","    device = get_device()\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    network = get_network(\n","        arch_type=arch_type,\n","        dataset=dataset,\n","        n_hidden=n_hidden,\n","        width=width,\n","        act_fn=act_fn\n","    )\n","    print(f\"network: {network}\\n\")\n","    model = BPN(network=network).to(device)\n","    parameter_scale = get_parameter_scale(\n","        dataset=dataset,\n","        n_hidden=n_hidden,\n","        optimiser=optimiser\n","    )\n","    for i, layer in enumerate(model.network):\n","        if (i+1) == len(network)-1 and init_type == \"other_saddle\":\n","            continue\n","        layer.apply(lambda m: init_weights(m, parameter_scale))\n","\n","    loss_fn = nn.MSELoss()\n","    optimizer = get_optimiser(id=optimiser, model=model, lr=LR)\n","\n","    # metrics\n","    batch_train_losses, epoch_test_losses = [], []\n","    batch_train_accs, epoch_test_accs = [], []\n","    grad_norms = []\n","    n_params = len(network)*2\n","    norms = {\n","        key: [[] for p in range(n_params)] for key in [\"params\", \"grads\"]\n","    }\n","    for i, param in enumerate(network.parameters()):\n","        norms[\"params\"][i].append(norm(param).item())\n","\n","    # record metrics at initialisation (batch 0)\n","    train_loader, test_loader = get_dataloaders(dataset_id=dataset)\n","    img_batch, label_batch = next(iter(train_loader))\n","    img_batch = img_batch.to(device)\n","    label_batch = label_batch.to(device)\n","    label_preds = model(img_batch)\n","    train_loss = loss_fn(label_preds, label_batch).item()\n","    batch_train_losses.append(train_loss)\n","    if dataset != \"toy_gaussian\":\n","        train_acc = accuracy(label_preds, label_batch)\n","        batch_train_accs.append(train_acc)\n","\n","    # plot landscape onto random directions\n","    if dataset == \"toy_gaussian\":\n","        for domain in DOMAINS:\n","            visualise_2D_loss_random_projections(\n","                model=model,\n","                input=img_batch,\n","                target=label_batch,\n","                domain=domain,\n","                device=device,\n","                save_dir=save_dir\n","            )\n","\n","    img_batch, label_batch = next(iter(test_loader))\n","    img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n","    label_preds = model(img_batch)\n","    test_loss = loss_fn(label_preds, label_batch).item()\n","    epoch_test_losses.append(test_loss)\n","    if dataset != \"toy_gaussian\":\n","        test_acc = accuracy(label_preds, label_batch)\n","        epoch_test_accs.append(test_acc)\n","\n","    global_batch_id = 0\n","    epoch_train_losses = []\n","    for epoch in range(1, max_epochs+1):\n","        print(f\"\\nEpoch {epoch}\\n-------------------------------\")\n","\n","        epoch_train_loss = 0\n","        for batch_id, (img_batch, label_batch) in enumerate(train_loader):\n","            img_batch = img_batch.to(device)\n","            label_batch = label_batch.to(device)\n","\n","            label_preds = model(img_batch)\n","            loss = loss_fn(label_preds, label_batch)\n","            epoch_train_loss += loss.item()\n","            if dataset != \"toy_gaussian\":\n","                train_acc = accuracy(label_preds, label_batch)\n","\n","            loss.backward()\n","            optimizer.step()\n","            grad_vec = get_gradient_vector(model=model)\n","            grad_norms.append(norm(grad_vec).item())\n","            for i, param in enumerate(network.parameters()):\n","                norms[\"params\"][i].append(norm(param).item())\n","                norms[\"grads\"][i].append(norm(param.grad).item())\n","\n","            model.zero_grad()\n","\n","            batch_train_losses.append(loss.item())\n","            if dataset != \"toy_gaussian\":\n","                batch_train_accs.append(train_acc)\n","\n","            global_batch_id += 1\n","\n","            if global_batch_id % LOG_BATCH_EVERY == 0:\n","                print(f\"Train loss: {loss.item():.5f} [{batch_id*len(img_batch)}/{len(train_loader.dataset)}]\")\n","\n","        test_loss, test_acc = (0, 0)\n","        for batch_id, (img_batch, label_batch) in enumerate(test_loader):\n","            img_batch = img_batch.to(device)\n","            label_batch = label_batch.to(device)\n","\n","            label_preds = model(img_batch)\n","            test_loss += loss_fn(label_preds, label_batch).item()\n","            if dataset != \"toy_gaussian\":\n","                test_acc += accuracy(label_preds, label_batch)\n","\n","        epoch_train_losses.append(epoch_train_loss / len(train_loader))\n","        epoch_test_losses.append(test_loss / len(test_loader))\n","        if dataset != \"toy_gaussian\":\n","            epoch_test_accs.append(test_acc / len(test_loader))\n","            print(f\"\\nAvg test accuracy: {test_acc / len(test_loader):.4f}\")\n","\n","        if init_type == \"standard\":\n","            if epoch > 1 and (epoch_train_losses[-2] - epoch_train_losses[-1]) < LOSS_TOLERANCE:\n","                break\n","\n","    print(f\"\\nTraining stopped at epoch {epoch}\\n\")\n","\n","    # plot losses, accuracies and norms\n","    if dataset == \"toy_gaussian\":\n","        plot_loss(\n","            loss=batch_train_losses,\n","            mode=\"train\",\n","            save_path=f\"{save_dir}/train_losses.pdf\"\n","        )\n","        plot_loss(\n","            loss=epoch_test_losses,\n","            mode=\"test\",\n","            save_path=f\"{save_dir}/test_losses.pdf\"\n","        )\n","    else:\n","        plot_loss_and_accuracy(\n","            loss=batch_train_losses,\n","            accuracy=batch_train_accs,\n","            mode=\"train\",\n","            save_path=f\"{save_dir}/train_losses_and_accs.pdf\"\n","        )\n","        plot_loss_and_accuracy(\n","            loss=epoch_test_losses,\n","            accuracy=epoch_test_accs,\n","            mode=\"test\",\n","            save_path=f\"{save_dir}/test_losses_and_accs.pdf\"\n","        )\n","    plot_norms(\n","        norms=norms[\"params\"],\n","        norm_type=\"parameters\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/parameters_norm\"\n","    )\n","    plot_norms(\n","        norms=norms[\"grads\"],\n","        norm_type=\"gradient\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/gradient_norm\"\n","    )\n","\n","    np.save(f\"{save_dir}/batch_train_losses.npy\", batch_train_losses)\n","    np.save(f\"{save_dir}/epoch_test_losses.npy\", epoch_test_losses)\n","\n","    np.save(f\"{save_dir}/batch_train_accs.npy\", batch_train_accs)\n","    np.save(f\"{save_dir}/epoch_test_accs.npy\", epoch_test_accs)\n","\n","    np.save(f\"{save_dir}/grad_norms.npy\", grad_norms)\n","\n","    return {\n","        \"losses\": {\n","            \"train\": batch_train_losses,\n","            \"test\": epoch_test_losses\n","        },\n","        \"accs\": {\n","            \"train\": batch_train_accs,\n","            \"test\": epoch_test_accs\n","        },\n","        \"grad_norms\": grad_norms\n","    }\n"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1722804745621,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"4SYNS19FAyCO"},"outputs":[],"source":["#@title PC train script\n","\n","\n","def train_pc(dataset, arch_type, n_hidden, width, act_fn, init_type, optimiser, seed, save_dir):\n","    print(\"Starting training with PC...\\n\")\n","    set_seed(seed)\n","    device = get_device()\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    network = get_network(\n","        arch_type=arch_type,\n","        dataset=dataset,\n","        n_hidden=n_hidden,\n","        width=width,\n","        act_fn=act_fn\n","    )\n","    model = PCN(network=network, dt=DT, device=device)\n","    parameter_scale = get_parameter_scale(\n","        dataset=dataset,\n","        n_hidden=n_hidden,\n","        optimiser=optimiser\n","    )\n","    for i, layer in enumerate(model.network):\n","        if (i+1) == len(network)-1 and init_type == \"other_saddle\":\n","            continue\n","        layer.apply(lambda m: init_weights(m, parameter_scale))\n","\n","    loss_fn = nn.MSELoss()\n","    optimizer = get_optimiser(id=optimiser, model=model, lr=LR)\n","\n","    # metrics\n","    batch_train_losses, epoch_test_losses = [], []\n","    batch_train_accs, epoch_test_accs = [], []\n","    grad_norms = []\n","    norms = {\n","        key: [[] for p in range(len(network)*2)] for key in [\"params\", \"grads\"]\n","    }\n","    for i, param in enumerate(network.parameters()):\n","        norms[\"params\"][i].append(norm(param).item())\n","\n","    # record train metrics at initialisation (batch 0)\n","    train_loader, test_loader = get_dataloaders(dataset_id=dataset)\n","    img_batch, label_batch = next(iter(train_loader))\n","    img_batch = img_batch.to(device)\n","    label_batch = label_batch.to(device)\n","    label_preds = model.forward(img_batch)\n","    train_loss = loss_fn(label_preds, label_batch).item()\n","    batch_train_losses.append(train_loss)\n","    if dataset != \"toy_gaussian\":\n","        train_acc = accuracy(label_preds, label_batch)\n","        batch_train_accs.append(train_acc)\n","\n","    # plot gradient inference dynamics\n","    _, grad_norms_iters = model.infer_train(\n","        obs=label_batch,\n","        prior=img_batch,\n","        n_iters=N_ITERS,\n","        record_grad_norms=True\n","    )\n","    plot_norms(\n","        norms=grad_norms_iters,\n","        norm_type=\"gradient\",\n","        mode=\"inference\",\n","        save_path=f\"{save_dir}/gradient_norm_inference\"\n","    )\n","\n","    # plot landscape onto random directions\n","    if dataset == \"toy_gaussian\":\n","        for domain in DOMAINS:\n","            visualise_2D_energy_random_projections(\n","                model=model,\n","                input=img_batch,\n","                target=label_batch,\n","                domain=domain,\n","                device=device,\n","                save_dir=save_dir\n","            )\n","\n","    img_batch, label_batch = next(iter(test_loader))\n","    img_batch = img_batch.to(device)\n","    label_batch = label_batch.to(device)\n","    label_preds = model.forward(img_batch)\n","    test_loss = loss_fn(label_preds, label_batch).item()\n","    epoch_test_losses.append(test_loss)\n","    if dataset != \"toy_gaussian\":\n","        test_acc = accuracy(label_preds, label_batch)\n","        epoch_test_accs.append(test_acc)\n","\n","    global_batch_id = 0\n","    epoch_train_losses = []\n","    for epoch in range(1, MAX_EPOCHS+1):\n","        print(f\"\\nEpoch {epoch}\\n-------------------------------\")\n","\n","        epoch_train_loss = 0\n","        for batch_id, (img_batch, label_batch) in enumerate(train_loader):\n","            img_batch = img_batch.to(device)\n","            label_batch = label_batch.to(device)\n","\n","            label_preds = model.forward(img_batch)\n","            loss = loss_fn(label_preds, label_batch).item()\n","            epoch_train_loss += loss\n","            if dataset != \"toy_gaussian\":\n","                train_acc = accuracy(label_preds, label_batch)\n","\n","            tot_energies = model.infer_train(\n","                obs=label_batch,\n","                prior=img_batch,\n","                n_iters=N_ITERS\n","            )\n","            optimizer.step()\n","            grad_vec = get_gradient_vector(model=model)\n","            grad_norms.append(norm(grad_vec).item())\n","            for i, param in enumerate(network.parameters()):\n","                norms[\"params\"][i].append(norm(param).item())\n","                norms[\"grads\"][i].append(norm(param.grad).item())\n","\n","            batch_train_losses.append(loss)\n","            if dataset != \"toy_gaussian\":\n","                batch_train_accs.append(train_acc)\n","\n","            global_batch_id += 1\n","\n","            if global_batch_id % LOG_BATCH_EVERY == 0:\n","                print(f\"Train loss: {loss:.7f} [{batch_id * len(img_batch)}/{len(train_loader.dataset)}]\")\n","\n","        test_loss, test_acc = (0, 0)\n","        for batch_id, (img_batch, label_batch) in enumerate(test_loader):\n","            img_batch = img_batch.to(device)\n","            label_batch = label_batch.to(device)\n","\n","            label_preds = model.forward(img_batch)\n","            test_loss += loss_fn(label_preds, label_batch).item()\n","            if dataset != \"toy_gaussian\":\n","                test_acc += accuracy(label_preds, label_batch)\n","\n","        epoch_train_losses.append(epoch_train_loss / len(train_loader))\n","        epoch_test_losses.append(test_loss / len(test_loader))\n","        if dataset != \"toy_gaussian\":\n","            epoch_test_accs.append(test_acc / len(test_loader))\n","            print(f\"\\nAvg test accuracy: {test_acc / len(test_loader):.4f}\")\n","\n","        if epoch > 1 and (epoch_train_losses[-2] - epoch_train_losses[-1]) < LOSS_TOLERANCE:\n","            print(f\"\\nTraining stopped at epoch {epoch}\\n\")\n","            break\n","\n","    # plot losses, accuracies and norms\n","    if dataset == \"toy_gaussian\":\n","        plot_loss(\n","            loss=batch_train_losses,\n","            mode=\"train\",\n","            save_path=f\"{save_dir}/train_losses.pdf\"\n","        )\n","        plot_loss(\n","            loss=epoch_test_losses,\n","            mode=\"test\",\n","            save_path=f\"{save_dir}/test_losses.pdf\"\n","        )\n","    else:\n","        plot_loss_and_accuracy(\n","            loss=batch_train_losses,\n","            accuracy=batch_train_accs,\n","            mode=\"train\",\n","            save_path=f\"{save_dir}/train_losses_and_accs.pdf\"\n","        )\n","        plot_loss_and_accuracy(\n","            loss=epoch_test_losses,\n","            accuracy=epoch_test_accs,\n","            mode=\"test\",\n","            save_path=f\"{save_dir}/test_losses_and_accs.pdf\"\n","        )\n","    plot_norms(\n","        norms=norms[\"params\"],\n","        norm_type=\"parameters\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/parameters_norm\"\n","    )\n","    plot_norms(\n","        norms=norms[\"grads\"],\n","        norm_type=\"gradient\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/gradient_norm\"\n","    )\n","\n","    np.save(f\"{save_dir}/batch_train_losses.npy\", batch_train_losses)\n","    np.save(f\"{save_dir}/epoch_test_losses.npy\", epoch_test_losses)\n","\n","    np.save(f\"{save_dir}/batch_train_accs.npy\", batch_train_accs)\n","    np.save(f\"{save_dir}/epoch_test_accs.npy\", epoch_test_accs)\n","\n","    np.save(f\"{save_dir}/grad_norms.npy\", grad_norms)\n","\n","    return {\n","        \"losses\": {\n","            \"train\": batch_train_losses,\n","            \"test\": epoch_test_losses\n","        },\n","        \"accs\": {\n","            \"train\": batch_train_accs,\n","            \"test\": epoch_test_accs\n","        },\n","        \"grad_norms\": grad_norms,\n","        \"n_epochs\": epoch\n","    }\n"]},{"cell_type":"code","execution_count":14,"metadata":{"cellView":"form","executionInfo":{"elapsed":6,"status":"ok","timestamp":1722804745621,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"},"user_tz":-60},"id":"MC9YHy36N5XJ"},"outputs":[],"source":["#@title Main script\n","\n","\n","def main():\n","    os.makedirs(DATA_DIR, exist_ok=True)\n","    for dataset in DATASETS:\n","        arch_type = get_architecture_type(dataset=dataset)\n","        for n_hidden, width in N_HIDDEN_WIDTHS[dataset]:\n","            for act_fn in ACT_FNS:\n","                for init_type in INIT_TYPES:\n","                    for optimiser in OPTIMISERS:\n","                        experiment_dir = setup_experiment(\n","                            results_dir=RESULTS_DIR,\n","                            dataset=dataset,\n","                            arch_type=arch_type,\n","                            n_hidden=n_hidden,\n","                            width=width,\n","                            act_fn=act_fn,\n","                            init_type=init_type,\n","                            optimiser=optimiser,\n","                            lr=LR\n","                        )\n","                        bp_train_losses_all_seeds = [[] for seed in range(N_SEEDS)]\n","                        bp_test_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","                        pc_train_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","                        pc_test_losses_all_seeds = bp_train_losses_all_seeds.copy()\n","\n","                        bp_grad_norms_all_seeds = bp_train_losses_all_seeds.copy()\n","                        pc_grad_norms_all_seeds = bp_train_losses_all_seeds.copy()\n","\n","                        if dataset != \"toy_gaussian\":\n","                            bp_train_accs_all_seeds = bp_train_losses_all_seeds.copy()\n","                            bp_test_accs_all_seeds = bp_train_losses_all_seeds.copy()\n","                            pc_train_accs_all_seeds = bp_train_losses_all_seeds.copy()\n","                            pc_test_accs_all_seeds = bp_train_losses_all_seeds.copy()\n","\n","                        for seed in range(N_SEEDS):\n","                            print(f\"\\nSeed {seed+1}/{N_SEEDS}...\")\n","\n","                            pc_metrics = train_pc(\n","                                dataset=dataset,\n","                                arch_type=arch_type,\n","                                n_hidden=n_hidden,\n","                                width=width,\n","                                act_fn=act_fn,\n","                                init_type=init_type,\n","                                optimiser=optimiser,\n","                                seed=seed,\n","                                save_dir=f\"{experiment_dir}/{str(seed)}/pc\",\n","                            )\n","                            max_epochs = pc_metrics[\"n_epochs\"] if dataset != \"toy_gaussian\" else MAX_EPOCHS\n","                            bp_metrics = train_bp(\n","                                dataset=dataset,\n","                                arch_type=arch_type,\n","                                n_hidden=n_hidden,\n","                                width=width,\n","                                act_fn=act_fn,\n","                                init_type=init_type,\n","                                optimiser=optimiser,\n","                                seed=seed,\n","                                max_epochs=max_epochs,\n","                                save_dir=f\"{experiment_dir}/{str(seed)}/bp\",\n","                            )\n","                            bp_train_losses_all_seeds[seed] = bp_metrics[\"losses\"][\"train\"]\n","                            bp_test_losses_all_seeds[seed] = bp_metrics[\"losses\"][\"test\"]\n","                            pc_train_losses_all_seeds[seed] = pc_metrics[\"losses\"][\"train\"]\n","                            pc_test_losses_all_seeds[seed] = pc_metrics[\"losses\"][\"test\"]\n","\n","                            bp_grad_norms_all_seeds[seed] = bp_metrics[\"grad_norms\"]\n","                            pc_grad_norms_all_seeds[seed] = pc_metrics[\"grad_norms\"]\n","\n","                            if dataset != \"toy_gaussian\":\n","                                bp_train_accs_all_seeds[seed] = bp_metrics[\"accs\"][\"train\"]\n","                                bp_test_accs_all_seeds[seed] = bp_metrics[\"accs\"][\"test\"]\n","                                pc_train_accs_all_seeds[seed] = pc_metrics[\"accs\"][\"train\"]\n","                                pc_test_accs_all_seeds[seed] = pc_metrics[\"accs\"][\"test\"]\n","\n","                        bp_train_loss_means, bp_train_loss_stds = compute_metric_stats(\n","                            metric=bp_train_losses_all_seeds\n","                        )\n","                        bp_test_loss_means, bp_test_loss_stds = compute_metric_stats(\n","                            metric=bp_test_losses_all_seeds\n","                        )\n","                        pc_train_loss_means, pc_train_loss_stds = compute_metric_stats(\n","                            metric=pc_train_losses_all_seeds\n","                        )\n","                        pc_test_loss_means, pc_test_loss_stds = compute_metric_stats(\n","                            metric=pc_test_losses_all_seeds\n","                        )\n","\n","                        plot_bp_and_pc_metric_stats(\n","                            means=[bp_train_loss_means, pc_train_loss_means],\n","                            stds=[bp_train_loss_stds, pc_train_loss_stds],\n","                            dataset=dataset,\n","                            optimiser=optimiser,\n","                            metric_title=\"$\\LARGE{\\mathcal{L}_{\\\\text{train}}}$\",\n","                            save_path=f\"{experiment_dir}/train_loss_stats.pdf\"\n","                        )\n","                        plot_bp_and_pc_metric_stats(\n","                            means=[bp_test_loss_means, pc_test_loss_means],\n","                            stds=[bp_test_loss_stds, pc_test_loss_stds],\n","                            dataset=dataset,\n","                            optimiser=optimiser,\n","                            metric_title=\"$\\LARGE{\\mathcal{L}_{\\\\text{test}}}$\",\n","                            save_path=f\"{experiment_dir}/test_loss_stats.pdf\"\n","                        )\n","\n","                        bp_grad_norm_means, bp_grad_norm_stds = compute_metric_stats(\n","                            metric=bp_grad_norms_all_seeds\n","                        )\n","                        pc_grad_norm_means, pc_grad_norm_stds = compute_metric_stats(\n","                            metric=pc_grad_norms_all_seeds\n","                        )\n","                        plot_bp_vs_pc_grad_norm_stats(\n","                            means=[bp_grad_norm_means, pc_grad_norm_means],\n","                            stds=[bp_grad_norm_stds, pc_grad_norm_stds],\n","                            dataset=dataset,\n","                            save_path=f\"{experiment_dir}/gradient_norm_stats.pdf\"\n","                        )\n","\n","                        if dataset != \"toy_gaussian\":\n","                            bp_train_acc_means, bp_train_acc_stds = compute_metric_stats(\n","                                metric=bp_train_accs_all_seeds\n","                            )\n","                            bp_test_acc_means, bp_test_acc_stds = compute_metric_stats(\n","                                metric=bp_test_accs_all_seeds\n","                            )\n","                            pc_train_acc_means, pc_train_acc_stds = compute_metric_stats(\n","                                metric=pc_train_accs_all_seeds\n","                            )\n","                            pc_test_acc_means, pc_test_acc_stds = compute_metric_stats(\n","                                metric=pc_test_accs_all_seeds\n","                            )\n","                            plot_bp_and_pc_metric_stats(\n","                                means=[bp_train_acc_means, pc_train_acc_means],\n","                                stds=[bp_train_acc_stds, pc_train_acc_stds],\n","                                dataset=dataset,\n","                                optimiser=optimiser,\n","                                metric_title=\"Train accuracy (%)\",\n","                                save_path=f\"{experiment_dir}/train_acc_stats.pdf\"\n","                            )\n","                            plot_bp_and_pc_metric_stats(\n","                                means=[bp_test_acc_means, pc_test_acc_means],\n","                                stds=[bp_test_acc_stds, pc_test_acc_stds],\n","                                dataset=dataset,\n","                                optimiser=optimiser,\n","                                metric_title=\"Test accuracy (%)\",\n","                                save_path=f\"{experiment_dir}/test_acc_stats.pdf\"\n","                            )\n"]},{"cell_type":"markdown","metadata":{"id":"GbFjLmSIVT3F"},"source":["## Run analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-f6nKL2rOJrn"},"outputs":[],"source":["main()"]},{"cell_type":"code","source":["import shutil\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hbQC07qKH2v","executionInfo":{"status":"ok","timestamp":1722804761099,"user_tz":-60,"elapsed":15483,"user":{"displayName":"Francesco Innocenti","userId":"11758167882892285303"}},"outputId":"3edfa2b7-039c-4c3d-914b-a499f3e0a013"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfZnIHiAHmYL"},"outputs":[],"source":["%%capture\n","!zip -r /content/results.zip /content/results"]},{"cell_type":"code","source":["colab_link = \"/content/results.zip\"\n","gdrive_link = \"/content/drive/MyDrive/\"\n","shutil.copy(colab_link, gdrive_link)"],"metadata":{"id":"wRrsLB7hKJih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"ddGxRslhKOjo"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[{"file_id":"1h54rW1LybJ2J_yiKPMUqcFLFy_3ErS04","timestamp":1713391026875}],"collapsed_sections":["U3Td0mTf9C3q"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

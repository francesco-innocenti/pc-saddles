{"cells":[{"cell_type":"markdown","metadata":{"id":"1b9WyJE3D3Il"},"source":["# Matrix completion\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tiGklA_e9Abi"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"XwkMH9mjx_dc","executionInfo":{"status":"ok","timestamp":1722888322509,"user_tz":-60,"elapsed":6727,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Installations\n","\n","\n","%%capture\n","!pip install -U kaleido\n"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","id":"tC-fTIbPAlIk","executionInfo":{"status":"ok","timestamp":1722888326274,"user_tz":-60,"elapsed":3769,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Imports\n","\n","\n","import os\n","import random\n","import subprocess\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import parameters_to_vector\n","import torch.optim as optim\n","from torch.linalg import norm\n","\n","import matplotlib.pyplot as plt\n","import plotly.graph_objs as go\n","import plotly.express as px\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","id":"8rlbd0rQGqVM","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":9,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Config\n","\n","\n","SEED = 0\n","N_HIDDEN = 3\n","WIDTH = 100\n","ACT_FNS = [\"linear\", \"tanh\", \"relu\"]\n","INIT_SCALE = 5e-3\n","LR = 1e-2\n","\n","RESULTS_DIR = \"results\"\n","\n","# PC hyperparameters\n","N_ITERS = 50\n","DT = 0.1\n","\n","# optimization hyperparameters\n","MAX_TRAIN_ITERS = 1000000\n","PRINT_EVERY = 2000\n","LOSS_TOL = 1e-2\n"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","id":"ATq_cOK_Ao8t","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":8,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Utils\n","\n","\n","def setup_experiment(\n","        results_dir,\n","        n_hidden,\n","        width,\n","        act_fn,\n","        init_scale,\n","        lr\n","    ):\n","    print(\n","f\"\"\"\n","Starting experiment with configuration:\n","\n","  N hidden: {n_hidden}\n","  Width: {width}\n","  Act fn: {act_fn}\n","  Init scale: {init_scale}\n","  Learning rate: {lr}\n","\n","\"\"\"\n",")\n","    return os.path.join(\n","        results_dir,\n","        f\"n_hidden_{n_hidden}\",\n","        f\"width_{width}\",\n","        act_fn,\n","        f\"init_scale_{init_scale}\",\n","        f\"lr_{lr}\"\n","    )\n","\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","\n","def get_device():\n","    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def get_fc_network(n_hidden, width, act_fn):\n","    layers = []\n","    for n in range(n_hidden):\n","        n_input = 10 if n == 0 else width\n","        if act_fn == \"linear\":\n","            hidden_layer = nn.Sequential(nn.Linear(n_input, width, bias=False))\n","        elif act_fn == \"tanh\":\n","            hidden_layer = nn.Sequential(\n","                nn.Linear(n_input, width, bias=False),\n","                nn.Tanh()\n","            )\n","        elif act_fn == \"relu\":\n","            hidden_layer = nn.Sequential(\n","                nn.Linear(n_input, width, bias=False),\n","                nn.ReLU(inplace=True)\n","            )\n","        layers.append(hidden_layer)\n","\n","    output_layer = nn.Sequential(nn.Linear(width, 10, bias=False))\n","    layers.append(output_layer)\n","    network = nn.Sequential(*layers)\n","    return network\n","\n","\n","def init_weights(module, param_scale):\n","    if isinstance(module, nn.Linear):\n","        nn.init.normal_(module.weight, mean=0., std=param_scale)\n","\n","\n","def get_gradient_vector(model):\n","    grad_vec = []\n","    for param in model.parameters():\n","        grad_vec.append(param.grad.view(-1))\n","    grad_vec = torch.cat(grad_vec)\n","    return grad_vec\n","\n","\n","def get_min_iter(lists):\n","    min_iter = 100000\n","    for i in lists:\n","        if len(i) < min_iter:\n","            min_iter = len(i)\n","    return min_iter\n","\n","\n","def get_min_iter_metrics(metrics):\n","    n_seeds = len(metrics)\n","    min_iter = get_min_iter(lists=metrics)\n","\n","    min_iter_metrics = np.zeros((n_seeds, min_iter))\n","    for seed in range(n_seeds):\n","        min_iter_metrics[seed, :] = metrics[seed][:min_iter]\n","\n","    return min_iter_metrics\n","\n","\n","def compute_metric_stats(metric):\n","    min_iter_metrics = get_min_iter_metrics(metrics=metric)\n","    metric_means = min_iter_metrics.mean(axis=0)\n","    metric_stds = min_iter_metrics.std(axis=0)\n","    return metric_means, metric_stds\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"IxZmNRPguIue","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":8,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Models\n","\n","\n","class BPN(nn.Module):\n","    def __init__(self, network):\n","        super(BPN, self).__init__()\n","        self.network = network\n","\n","    def forward(self, x):\n","        x = self.network(x)\n","        return x\n","\n","    def get_weights(self):\n","        weights = []\n","        for param in self.network.parameters():\n","            if len(param.shape) > 1:\n","                weights.append(param.cpu().detach().numpy())\n","        return weights\n","\n","\n","\n","class PCN(object):\n","    def __init__(self, network, dt, device=\"cpu\"):\n","        self.network = network.to(device)\n","        self.n_layers = len(self.network)\n","        self.n_nodes = self.n_layers + 1\n","        self.dt = dt\n","        self.n_params = sum(\n","            p.numel() for p in network.parameters() if p.requires_grad\n","        )\n","        self.device = device\n","\n","    def reset(self):\n","        self.zero_grad()\n","        self.preds = [None] * self.n_nodes\n","        self.errs = [None] * self.n_nodes\n","        self.xs = [None] * self.n_nodes\n","\n","    def reset_xs(self, prior, init_std):\n","        self.set_prior(prior)\n","        self.propagate_xs()\n","        for l in range(self.n_layers):\n","            self.xs[l] = torch.empty(self.xs[l].shape).normal_(\n","                mean=0,\n","                std=init_std\n","            ).to(self.device)\n","\n","    def set_obs(self, obs):\n","        self.xs[-1] = obs.clone()\n","\n","    def set_prior(self, prior):\n","        self.xs[0] = prior.clone()\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","    def propagate_xs(self):\n","        for l in range(1, self.n_layers):\n","            self.xs[l] = self.network[l - 1](self.xs[l - 1])\n","\n","    def infer_train(\n","            self,\n","            obs,\n","            prior,\n","            n_iters,\n","            record_grad_norms=False,\n","        ):\n","        self.reset()\n","        self.set_prior(prior)\n","        self.propagate_xs()\n","        self.set_obs(obs)\n","\n","        if record_grad_norms:\n","            grad_norms_iters = [np.zeros(n_iters) for p in range(len(self.network)*2)]\n","\n","        dt = self.dt\n","        tot_energy_iters = []\n","        for t in range(n_iters):\n","            self.network.zero_grad()\n","            self.preds[-1] = self.network[self.n_layers - 1](self.xs[self.n_layers - 1])\n","            self.errs[-1] = self.xs[-1] - self.preds[-1]\n","\n","            for l in reversed(range(1, self.n_layers)):\n","                self.preds[l] = self.network[l - 1](self.xs[l - 1])\n","                self.errs[l] = self.xs[l] - self.preds[l]\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[l],\n","                    self.xs[l],\n","                    self.errs[l + 1]\n","                )\n","                with torch.no_grad():\n","                    dx = epsdfdx - self.errs[l]\n","                    self.xs[l] = self.xs[l] + self.dt * dx\n","\n","            tot_energy_iters.append(self.compute_tot_energy())\n","            if t > 0 and tot_energy_iters[t] >= tot_energy_iters[t-1] and self.dt > 0.025:\n","                self.dt /= 2\n","                if self.dt <= 0.025:\n","                    self.dt = dt\n","                    break\n","\n","            if (t+1) != n_iters:\n","                self.clear_grads()\n","\n","        if record_grad_norms:\n","            self.set_grads(\n","                grad_norms_iters=grad_norms_iters,\n","                t=t\n","            )\n","        else:\n","            self.set_grads()\n","\n","        if record_grad_norms:\n","            return tot_energy_iters, grad_norms_iters\n","        else:\n","            return tot_energy_iters\n","\n","    def infer_test(\n","            self,\n","            obs,\n","            prior,\n","            n_iters,\n","            update_prior=True,\n","            update_obs=False,\n","            init_std=0.05,\n","        ):\n","\n","        self.reset()\n","        self.reset_xs(prior, init_std)\n","        if not update_prior:\n","            self.set_prior(prior)\n","        self.set_obs(obs)\n","\n","        for t in range(n_iters):\n","            self.network.zero_grad()\n","            self.preds[-1] = self.network[self.n_layers - 1](self.xs[self.n_layers - 1])\n","            self.errs[-1] = self.xs[-1] - self.preds[-1]\n","\n","            if update_obs:\n","                with torch.no_grad():\n","                    self.xs[-1] = self.xs[-1] + self.dt * (- self.errs[-1])\n","\n","            for l in reversed(range(1, self.n_layers)):\n","                self.preds[l] = self.network[l - 1](self.xs[l - 1])\n","                self.errs[l] = self.xs[l] - self.preds[l]\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[l],\n","                    self.xs[l],\n","                    self.errs[l + 1]\n","                )\n","                with torch.no_grad():\n","                    dx = epsdfdx - self.errs[l]\n","                    self.xs[l] = self.xs[l] + self.dt * dx\n","\n","            if update_prior:\n","                _, epsdfdx = torch.autograd.functional.vjp(\n","                    self.network[0],\n","                    self.xs[0],\n","                    self.errs[1]\n","                )\n","                with torch.no_grad():\n","                    self.xs[0] = self.xs[0] + self.dt * epsdfdx\n","\n","            if (t+1) != n_iters:\n","                self.clear_grads()\n","\n","        if update_prior:\n","            return self.xs[0]\n","        elif update_obs:\n","            return self.xs[-1]\n","\n","    def set_grads(self, grad_norms_iters=None, t=None):\n","        n = 0\n","        for l in range(self.n_layers):\n","            for i, param in enumerate(self.network[l].parameters()):\n","                dparam = torch.autograd.grad(\n","                    self.preds[l + 1],\n","                    param,\n","                    - self.errs[l + 1],\n","                    allow_unused=True,\n","                    retain_graph=True\n","                )[0]\n","                param.grad = dparam.clone()\n","                if grad_norms_iters is not None:\n","                    grad_norm = torch.linalg.norm(dparam)\n","                    grad_norms_iters[n][t] = grad_norm.item()\n","                    n += 1\n","\n","    def zero_grad(self):\n","        self.network.zero_grad()\n","\n","    def clear_grads(self):\n","        with torch.no_grad():\n","            for l in range(1, self.n_nodes):\n","                self.preds[l] = self.preds[l].clone()\n","                self.errs[l] = self.errs[l].clone()\n","                self.xs[l] = self.xs[l].clone()\n","\n","    def save_weights(self, path):\n","        torch.save(self.network.state_dict(), path)\n","\n","    def load_weights(self, path):\n","        self.network.load_state_dict(torch.load(path))\n","\n","    def get_weights(self):\n","        weights = []\n","        for param in self.network.parameters():\n","            if len(param.shape) > 1:\n","                weights.append(param.cpu().detach().numpy())\n","        return weights\n","\n","    def compute_tot_energy(self):\n","        energy = 0.\n","        for err in self.errs:\n","            if err is not None:\n","                energy += (err**2).sum()\n","        return energy.item()\n","\n","    def parameters(self):\n","        return self.network.parameters()\n","\n","    def __str__(self):\n","        return f\"PCN(\\n{self.network}\\n\"\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","id":"eMJJ1NyknHyU","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Plotting\n","\n","\n","def plot_loss(loss, mode, save_path):\n","    n_train_iters = len(loss)\n","    train_iters = [b+1 for b in range(n_train_iters)]\n","\n","    loss_color = \"#EF553B\"\n","    fig = go.Figure()\n","    fig.add_trace(\n","        go.Scatter(\n","            x=train_iters,\n","            y=loss,\n","            mode=\"lines\",\n","            line=dict(width=2, color=loss_color),\n","            showlegend=False\n","        )\n","    )\n","    fig.update_layout(\n","        height=300,\n","        width=400,\n","        xaxis=dict(\n","            title=\"Batch\" if mode == \"train\" else \"Epoch\",\n","            tickvals=[1, int(train_iters[-1]/2), train_iters[-1]],\n","            ticktext=[1, int(train_iters[-1]/2), train_iters[-1]]\n","        ),\n","        yaxis=dict(\n","            title=f\"$\\Large{{\\mathcal{{L}}_{{{mode}}}}}$\"\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_norms(norms, norm_type, mode, save_path):\n","    n_params = len(norms)\n","    n_iterations = len(norms[0])\n","    iterations = [b+1 for b in range(n_iterations)]\n","\n","    fig = go.Figure()\n","    weights_id = [\n","        f\"$W_{i+1}$\" if norm_type == \"parameters\" else f\"$\\partial W_{i+1}$\" for i in range(n_params)\n","    ]\n","    colors = px.colors.qualitative.Plotly[2:]\n","    for weight_norms, weight_id, color in zip(norms, weights_id, colors):\n","        fig.add_traces(\n","            go.Scatter(\n","                x=iterations,\n","                y=weight_norms,\n","                name=weight_id,\n","                mode=\"lines\",\n","                line=dict(width=2, color=color)\n","            )\n","        )\n","\n","    fig_width = 300 if norm_type == \"parameters\" else 400\n","    xaxis_title = \"Training iteration\" if mode == \"learning\" else \"Inference iteration\"\n","    fig.update_layout(\n","        height=300,\n","        width=fig_width,\n","        xaxis=dict(\n","            title=xaxis_title,\n","            tickvals=[1, int(iterations[-1]/2), iterations[-1]],\n","            ticktext=[1, int(iterations[-1]/2), iterations[-1]],\n","        ),\n","        yaxis=dict(\n","            title=\"$\\Large{||W||_F}$\" if norm_type == \"parameters\" else \"$\\Large{||\\partial W||_F}$\",\n","            nticks=3\n","        ),\n","        font=dict(size=16),\n","    )\n","    fig.write_image(f\"{save_path}_weight.pdf\")\n","\n","\n","def plot_bp_and_pc_losses(bp_losses, pc_losses, rank_iters, log_x, save_path):\n","    rank1_iter, rank2_iter = rank_iters\n","    max_train_iter = len(bp_losses) if len(bp_losses) >= rank2_iter+len(pc_losses[2]) else rank2_iter+len(pc_losses[2])\n","    bp_color, pc_color = \"#EF553B\", \"#636EFA\"\n","\n","    fig = go.Figure()\n","    fig.add_traces(\n","        go.Scatter(\n","            y=bp_losses,\n","            name=\"BP\",\n","            mode=\"lines+markers\",\n","            line=dict(width=3, color=bp_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            y=pc_losses[0],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            x=[iter for iter in range(rank1_iter, rank1_iter+len(pc_losses[1]))],\n","            y=pc_losses[1],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            showlegend=False,\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            x=[iter for iter in range(rank2_iter, rank2_iter+len(pc_losses[2]))],\n","            y=pc_losses[2],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            showlegend=False,\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    if log_x:\n","        fig.update_layout(\n","            xaxis=dict(\n","                title=\"Training iteration (log)\",\n","                type=\"log\",\n","                exponentformat=\"power\",\n","                dtick=1\n","            )\n","        )\n","    else:\n","        fig.update_layout(\n","            xaxis=dict(\n","                title=\"Training iteration\",\n","                tickvals=[1, int(max_train_iter/2), max_train_iter],\n","            ticktext=[1, int(max_train_iter/2), max_train_iter]\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        yaxis=dict(\n","            title=\"Train loss (log)\",\n","            type=\"log\",\n","            exponentformat=\"power\",\n","            dtick=1\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n","\n","def plot_bp_and_pc_norms(bp_norms, pc_norms, rank_iters, log_x, save_path):\n","    rank1_iter, rank2_iter = rank_iters\n","    max_train_iter = len(bp_norms) if len(bp_norms) >= rank2_iter+len(pc_norms[2]) else rank2_iter+len(pc_norms[2])\n","    bp_color, pc_color = \"#EF553B\", \"#636EFA\"\n","\n","    fig = go.Figure()\n","    fig.add_traces(\n","        go.Scatter(\n","            y=bp_norms,\n","            name=\"BP\",\n","            mode=\"lines+markers\",\n","            line=dict(width=3, color=bp_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            y=pc_norms[0],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            x=[iter for iter in range(rank1_iter, rank1_iter+len(pc_norms[1]))],\n","            y=pc_norms[1],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            showlegend=False,\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    fig.add_traces(\n","        go.Scatter(\n","            x=[iter for iter in range(rank2_iter, rank2_iter+len(pc_norms[2]))],\n","            y=pc_norms[2],\n","            name=\"PC\",\n","            mode=\"lines+markers\",\n","            showlegend=False,\n","            line=dict(width=3, color=pc_color)\n","        )\n","    )\n","    if log_x:\n","        fig.update_layout(\n","            xaxis=dict(\n","                title=\"Training iteration (log)\",\n","                type=\"log\",\n","                exponentformat=\"power\",\n","                dtick=1\n","            )\n","        )\n","    else:\n","        fig.update_layout(\n","            xaxis=dict(\n","                title=\"Training iteration\",\n","                tickvals=[1, int(max_train_iter/2), max_train_iter],\n","            ticktext=[1, int(max_train_iter/2), max_train_iter]\n","            )\n","        )\n","\n","    fig.update_layout(\n","        height=300,\n","        width=350,\n","        yaxis=dict(\n","            title=\"$\\Large{||\\partial \\\\theta||_2}$\"\n","        ),\n","        font=dict(size=16)\n","    )\n","    fig.write_image(save_path)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U3Td0mTf9C3q"},"source":["## Scripts"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","id":"LsE8rFp2b_N5","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title BP train script\n","\n","\n","def train_bp(act_fn, save_dir):\n","    print(\"Starting training with BP...\\n\")\n","    set_seed(SEED)\n","    device = get_device()\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    network = get_fc_network(\n","        n_hidden=N_HIDDEN,\n","        width=WIDTH,\n","        act_fn=act_fn\n","    )\n","    print(f\"network: {network}\\n\")\n","    model = BPN(network).to(device)\n","    model.apply(lambda m: init_weights(m, INIT_SCALE))\n","\n","    mse_loss = nn.MSELoss()\n","    optimizer = optim.SGD(model.parameters(), lr=LR)\n","\n","    # metrics\n","    train_losses, test_losses = [], []\n","    grad_norms = []\n","    n_params = len(network)*2\n","    norms = {\n","        key: [[] for p in range(n_params)] for key in [\"params\", \"grads\"]\n","    }\n","    for i, param in enumerate(network.parameters()):\n","        norms[\"params\"][i].append(norm(param).item())\n","\n","    # data\n","    A = torch.randn(size=(10, 3))\n","    B = torch.randn(size=(3, 10))\n","\n","    target_matrix = (A @ B).to(device)\n","    mask = (torch.rand(10, 10) > 0.2).float().to(device)\n","    masked_matrix = target_matrix * mask\n","\n","    if act_fn == \"linear\":\n","        rank1_loss_thresh, rank2_loss_thresh = 1.1, 0.15\n","    elif act_fn == \"tanh\":\n","        rank1_loss_thresh, rank2_loss_thresh = 1.2, 0.15\n","    elif act_fn == \"relu\":\n","        rank1_loss_thresh, rank2_loss_thresh = 2.0, 0.6  # .5\n","\n","    rank1_iter, rank2_iter = None, None\n","    for iter in range(MAX_TRAIN_ITERS):\n","        output = model(masked_matrix)\n","        loss = mse_loss(output * mask, target_matrix * mask)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        grad_vec = get_gradient_vector(model=model)\n","        grad_norms.append(norm(grad_vec).item())\n","        for i, param in enumerate(network.parameters()):\n","            norms[\"params\"][i].append(norm(param).item())\n","            norms[\"grads\"][i].append(norm(param.grad).item())\n","\n","        model.zero_grad()\n","        optimizer.zero_grad()\n","        train_losses.append(loss.item())\n","\n","        if iter % PRINT_EVERY == 0:\n","            print(f\"Train loss: {loss:.7f} [{iter}/{MAX_TRAIN_ITERS}]\")\n","\n","        if rank1_iter is None and loss < rank1_loss_thresh:\n","            rank1_iter = iter\n","            torch.save(model.network.state_dict(), save_dir + f\"/weights_rank_1.pth\")\n","\n","        if rank2_iter is None and loss < rank2_loss_thresh:\n","            rank2_iter = iter\n","            torch.save(model.network.state_dict(), save_dir + f\"/weights_rank_2.pth\")\n","\n","        if loss <= LOSS_TOL:\n","            break\n","\n","    # plot losses and norms\n","    plot_loss(\n","        loss=train_losses,\n","        mode=\"train\",\n","        save_path=f\"{save_dir}/train_losses.pdf\"\n","    )\n","    plot_norms(\n","        norms=norms[\"params\"],\n","        norm_type=\"parameters\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/parameters_norm\"\n","    )\n","    plot_norms(\n","        norms=norms[\"grads\"],\n","        norm_type=\"gradient\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/gradient_norm\"\n","    )\n","    np.save(f\"{save_dir}/train_losses.npy\", train_losses)\n","    np.save(f\"{save_dir}/grad_norms.npy\", grad_norms)\n","\n","    return train_losses, grad_norms, rank1_iter, rank2_iter\n"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","id":"4SYNS19FAyCO","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title PC train script\n","\n","\n","def train_pc(act_fn, bp_save_dir, save_dir):\n","    print(\"\\nStarting training with PC...\\n\")\n","    set_seed(SEED)\n","    device = get_device()\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    network = get_fc_network(\n","        n_hidden=N_HIDDEN,\n","        width=WIDTH,\n","        act_fn=act_fn\n","    )\n","    model = PCN(network=network, dt=DT, device=device)\n","    if \"rank0\" in save_dir:\n","        print(f\"rank0\")\n","        model.network.apply(lambda m: init_weights(m, INIT_SCALE))\n","    if \"rank1\" in save_dir:\n","        print(f\"rank1\")\n","        model.network.load_state_dict(\n","            torch.load(f\"{bp_save_dir}/weights_rank_1.pth\")\n","        )\n","    if \"rank2\" in save_dir:\n","        print(f\"rank2\")\n","        model.network.load_state_dict(\n","            torch.load(f\"{bp_save_dir}/weights_rank_2.pth\")\n","        )\n","\n","    mse_loss = nn.MSELoss()\n","    optimizer = optim.SGD(params=model.parameters(), lr=LR)\n","\n","    # metrics\n","    train_losses, test_losses = [], []\n","    grad_norms = []\n","    norms = {\n","        key: [[] for p in range(len(network)*2)] for key in [\"params\", \"grads\"]\n","    }\n","    for i, param in enumerate(network.parameters()):\n","        norms[\"params\"][i].append(norm(param).item())\n","\n","    # data\n","    A = torch.randn(size=(10, 3))\n","    B = torch.randn(size=(3, 10))\n","\n","    target_matrix = (A @ B).to(device)\n","    mask = (torch.rand(10, 10) > 0.2).float().to(device)\n","    masked_matrix = target_matrix * mask\n","\n","    for iter in range(MAX_TRAIN_ITERS):\n","        output = model.forward(masked_matrix)\n","        loss = mse_loss(output * mask, target_matrix * mask).item()\n","\n","        tot_energies = model.infer_train(\n","            obs=target_matrix,\n","            prior=masked_matrix,\n","            n_iters=N_ITERS\n","        )\n","        optimizer.step()\n","        grad_vec = get_gradient_vector(model=model)\n","        grad_norms.append(norm(grad_vec).item())\n","        for i, param in enumerate(network.parameters()):\n","            norms[\"params\"][i].append(norm(param).item())\n","            norms[\"grads\"][i].append(norm(param.grad).item())\n","\n","        train_losses.append(loss)\n","\n","        if iter % PRINT_EVERY == 0:\n","            print(f\"Train loss: {loss:.7f} [{iter}/{MAX_TRAIN_ITERS}]\")\n","\n","        if loss <= LOSS_TOL:\n","            break\n","\n","    # plot losses and norms\n","    plot_loss(\n","        loss=train_losses,\n","        mode=\"train\",\n","        save_path=f\"{save_dir}/train_losses.pdf\"\n","    )\n","    plot_norms(\n","        norms=norms[\"params\"],\n","        norm_type=\"parameters\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/parameters_norm\"\n","    )\n","    plot_norms(\n","        norms=norms[\"grads\"],\n","        norm_type=\"gradient\",\n","        mode=\"learning\",\n","        save_path=f\"{save_dir}/gradient_norm\"\n","    )\n","    np.save(f\"{save_dir}/train_losses.npy\", train_losses)\n","    np.save(f\"{save_dir}/grad_norms.npy\", grad_norms)\n","\n","    return train_losses, grad_norms\n"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","id":"MC9YHy36N5XJ","executionInfo":{"status":"ok","timestamp":1722888326275,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["#@title Main script\n","\n","\n","def main():\n","    for act_fn in ACT_FNS:\n","        experiment_dir = setup_experiment(\n","            results_dir=RESULTS_DIR,\n","            n_hidden=N_HIDDEN,\n","            width=WIDTH,\n","            act_fn=act_fn,\n","            init_scale=INIT_SCALE,\n","            lr=LR\n","        )\n","\n","        bp_save_dir = f\"{experiment_dir}/{str(SEED)}/bp\"\n","        bp_train_losses, bp_grad_norms, rank1_iter, rank2_iter = train_bp(\n","            act_fn=act_fn,\n","            save_dir=bp_save_dir,\n","        )\n","        pc_train_losses_rank0, pc_grad_norms_rank0 = train_pc(\n","            act_fn=act_fn,\n","            bp_save_dir=bp_save_dir,\n","            save_dir=f\"{experiment_dir}/{str(SEED)}/pc_rank0\",\n","        )\n","        pc_train_losses_rank1, pc_grad_norms_rank1 = train_pc(\n","            act_fn=act_fn,\n","            bp_save_dir=bp_save_dir,\n","            save_dir=f\"{experiment_dir}/{str(SEED)}/pc_rank1\",\n","        )\n","        pc_train_losses_rank2, pc_grad_norms_rank2 = train_pc(\n","            act_fn=act_fn,\n","            bp_save_dir=bp_save_dir,\n","            save_dir=f\"{experiment_dir}/{str(SEED)}/pc_rank2\",\n","        )\n","\n","        plot_bp_and_pc_losses(\n","            bp_losses=bp_train_losses,\n","            pc_losses=[\n","                pc_train_losses_rank0,\n","                pc_train_losses_rank1,\n","                pc_train_losses_rank2\n","            ],\n","            rank_iters=[rank1_iter, rank2_iter],\n","            log_x=False,\n","            save_path=f\"{experiment_dir}/train_losses.pdf\"\n","        )\n","        plot_bp_and_pc_losses(\n","            bp_losses=bp_train_losses,\n","            pc_losses=[\n","                pc_train_losses_rank0,\n","                pc_train_losses_rank1,\n","                pc_train_losses_rank2\n","            ],\n","            rank_iters=[rank1_iter, rank2_iter],\n","            log_x=True,\n","            save_path=f\"{experiment_dir}/train_losses_log_log.pdf\"\n","        )\n","\n","        plot_bp_and_pc_norms(\n","            bp_norms=bp_grad_norms,\n","            pc_norms=[\n","                pc_grad_norms_rank0,\n","                pc_grad_norms_rank1,\n","                pc_grad_norms_rank2\n","            ],\n","            rank_iters=[rank1_iter, rank2_iter],\n","            log_x=False,\n","            save_path=f\"{experiment_dir}/grad_norms.pdf\"\n","        )\n","        plot_bp_and_pc_norms(\n","            bp_norms=bp_grad_norms,\n","            pc_norms=[\n","                pc_grad_norms_rank0,\n","                pc_grad_norms_rank1,\n","                pc_grad_norms_rank2\n","            ],\n","            rank_iters=[rank1_iter, rank2_iter],\n","            log_x=True,\n","            save_path=f\"{experiment_dir}/grad_norms_log.pdf\"\n","        )\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GbFjLmSIVT3F"},"source":["## Run analysis"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-f6nKL2rOJrn","outputId":"90e0f8c5-89b6-47b0-a975-96c4dd0efd14","executionInfo":{"status":"ok","timestamp":1722889151820,"user_tz":-60,"elapsed":825551,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting experiment with configuration:\n","\n","  N hidden: 3\n","  Width: 100\n","  Act fn: linear\n","  Init scale: 0.005\n","  Learning rate: 0.01\n","\n","\n","Starting training with BP...\n","\n","network: Sequential(\n","  (0): Sequential(\n","    (0): Linear(in_features=10, out_features=100, bias=False)\n","  )\n","  (1): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","  )\n","  (2): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","  )\n","  (3): Sequential(\n","    (0): Linear(in_features=100, out_features=10, bias=False)\n","  )\n",")\n","\n","Train loss: 3.4457870 [0/1000000]\n","Train loss: 3.4457674 [2000/1000000]\n","Train loss: 3.4457097 [4000/1000000]\n","Train loss: 3.4445028 [6000/1000000]\n","Train loss: 1.0968647 [8000/1000000]\n","Train loss: 1.0959240 [10000/1000000]\n","Train loss: 1.0955961 [12000/1000000]\n","Train loss: 1.0946143 [14000/1000000]\n","Train loss: 0.1425766 [16000/1000000]\n","Train loss: 0.1325606 [18000/1000000]\n","Train loss: 0.1277779 [20000/1000000]\n","Train loss: 0.1263119 [22000/1000000]\n","Train loss: 0.1259822 [24000/1000000]\n","Train loss: 0.1258731 [26000/1000000]\n","Train loss: 0.1258141 [28000/1000000]\n","Train loss: 0.1257706 [30000/1000000]\n","Train loss: 0.1257339 [32000/1000000]\n","Train loss: 0.1257013 [34000/1000000]\n","Train loss: 0.1256721 [36000/1000000]\n","Train loss: 0.1256459 [38000/1000000]\n","Train loss: 0.1256223 [40000/1000000]\n","Train loss: 0.1256009 [42000/1000000]\n","Train loss: 0.1255817 [44000/1000000]\n","Train loss: 0.1255643 [46000/1000000]\n","Train loss: 0.1255486 [48000/1000000]\n","Train loss: 0.1255344 [50000/1000000]\n","Train loss: 0.1255215 [52000/1000000]\n","Train loss: 0.1255097 [54000/1000000]\n","Train loss: 0.1254989 [56000/1000000]\n","Train loss: 0.1254890 [58000/1000000]\n","Train loss: 0.1254798 [60000/1000000]\n","Train loss: 0.1254711 [62000/1000000]\n","Train loss: 0.1254629 [64000/1000000]\n","Train loss: 0.1254549 [66000/1000000]\n","Train loss: 0.1254469 [68000/1000000]\n","Train loss: 0.1254388 [70000/1000000]\n","Train loss: 0.1254303 [72000/1000000]\n","Train loss: 0.1254207 [74000/1000000]\n","Train loss: 0.1254095 [76000/1000000]\n","Train loss: 0.1253954 [78000/1000000]\n","Train loss: 0.1253763 [80000/1000000]\n","Train loss: 0.1253477 [82000/1000000]\n","Train loss: 0.1252998 [84000/1000000]\n","Train loss: 0.1252057 [86000/1000000]\n","Train loss: 0.1249713 [88000/1000000]\n","Train loss: 0.1240445 [90000/1000000]\n","Train loss: 0.1080109 [92000/1000000]\n","\n","Starting training with PC...\n","\n","rank0\n","Train loss: 3.4457870 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank1\n","Train loss: 1.0999867 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank2\n","Train loss: 0.1499545 [0/1000000]\n","\n","Starting experiment with configuration:\n","\n","  N hidden: 3\n","  Width: 100\n","  Act fn: tanh\n","  Init scale: 0.005\n","  Learning rate: 0.01\n","\n","\n","Starting training with BP...\n","\n","network: Sequential(\n","  (0): Sequential(\n","    (0): Linear(in_features=10, out_features=100, bias=False)\n","    (1): Tanh()\n","  )\n","  (1): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","    (1): Tanh()\n","  )\n","  (2): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","    (1): Tanh()\n","  )\n","  (3): Sequential(\n","    (0): Linear(in_features=100, out_features=10, bias=False)\n","  )\n",")\n","\n","Train loss: 3.4457870 [0/1000000]\n","Train loss: 3.4457674 [2000/1000000]\n","Train loss: 3.4457104 [4000/1000000]\n","Train loss: 3.4445996 [6000/1000000]\n","Train loss: 1.1046042 [8000/1000000]\n","Train loss: 1.0933250 [10000/1000000]\n","Train loss: 0.1371823 [12000/1000000]\n","Train loss: 0.1169120 [14000/1000000]\n","Train loss: 0.0792434 [16000/1000000]\n","Train loss: 0.0460626 [18000/1000000]\n","Train loss: 0.0294796 [20000/1000000]\n","Train loss: 0.0194087 [22000/1000000]\n","Train loss: 0.0134491 [24000/1000000]\n","\n","Starting training with PC...\n","\n","rank0\n","Train loss: 3.4457870 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank1\n","Train loss: 1.1995432 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank2\n","Train loss: 0.1499305 [0/1000000]\n","\n","Starting experiment with configuration:\n","\n","  N hidden: 3\n","  Width: 100\n","  Act fn: relu\n","  Init scale: 0.005\n","  Learning rate: 0.01\n","\n","\n","Starting training with BP...\n","\n","network: Sequential(\n","  (0): Sequential(\n","    (0): Linear(in_features=10, out_features=100, bias=False)\n","    (1): ReLU(inplace=True)\n","  )\n","  (1): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","    (1): ReLU(inplace=True)\n","  )\n","  (2): Sequential(\n","    (0): Linear(in_features=100, out_features=100, bias=False)\n","    (1): ReLU(inplace=True)\n","  )\n","  (3): Sequential(\n","    (0): Linear(in_features=100, out_features=10, bias=False)\n","  )\n",")\n","\n","Train loss: 3.4457850 [0/1000000]\n","Train loss: 3.4457839 [2000/1000000]\n","Train loss: 3.4457829 [4000/1000000]\n","Train loss: 3.4457817 [6000/1000000]\n","Train loss: 3.4457812 [8000/1000000]\n","Train loss: 3.4457793 [10000/1000000]\n","Train loss: 3.4457772 [12000/1000000]\n","Train loss: 3.4457738 [14000/1000000]\n","Train loss: 3.4457681 [16000/1000000]\n","Train loss: 3.4457560 [18000/1000000]\n","Train loss: 3.4457219 [20000/1000000]\n","Train loss: 3.4455285 [22000/1000000]\n","Train loss: 1.9772176 [24000/1000000]\n","Train loss: 1.9717609 [26000/1000000]\n","Train loss: 1.9716457 [28000/1000000]\n","Train loss: 1.9716167 [30000/1000000]\n","Train loss: 1.9715910 [32000/1000000]\n","Train loss: 1.9715492 [34000/1000000]\n","Train loss: 1.9714192 [36000/1000000]\n","Train loss: 1.9688416 [38000/1000000]\n","Train loss: 0.5227159 [40000/1000000]\n","Train loss: 0.4463926 [42000/1000000]\n","Train loss: 0.0887654 [44000/1000000]\n","Train loss: 0.0778818 [46000/1000000]\n","Train loss: 0.0734234 [48000/1000000]\n","Train loss: 0.0688392 [50000/1000000]\n","Train loss: 0.0637351 [52000/1000000]\n","Train loss: 0.0588593 [54000/1000000]\n","Train loss: 0.0543421 [56000/1000000]\n","Train loss: 0.0495902 [58000/1000000]\n","Train loss: 0.0417690 [60000/1000000]\n","Train loss: 0.0193488 [62000/1000000]\n","\n","Starting training with PC...\n","\n","rank0\n","Train loss: 3.4457850 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank1\n","Train loss: 1.9988170 [0/1000000]\n","\n","Starting training with PC...\n","\n","rank2\n","Train loss: 0.5995589 [0/1000000]\n"]}],"source":["main()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"PfZnIHiAHmYL","executionInfo":{"status":"ok","timestamp":1722889152364,"user_tz":-60,"elapsed":563,"user":{"displayName":"Francesco Innocenti","userId":"12349586889561460703"}}},"outputs":[],"source":["%%capture\n","!zip -r /content/results.zip /content/results"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["GbFjLmSIVT3F"],"gpuType":"L4","provenance":[{"file_id":"1h54rW1LybJ2J_yiKPMUqcFLFy_3ErS04","timestamp":1713391026875}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
